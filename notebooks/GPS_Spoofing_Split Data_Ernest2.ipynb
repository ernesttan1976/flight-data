{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d691c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m venv venv\n",
    "# # !venv/scripts/activate\n",
    "%pip install pandas numpy matplotlib seaborn plotly geopandas scipy scikit-learn statsmodels requests aiohttp shapely ipython h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7ee0f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Statistical Models\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "from pandas import read_csv\n",
    "import concurrent.futures\n",
    "import psutil\n",
    "\n",
    "from datetime import datetime\n",
    "import re\n",
    "import csv\n",
    "\n",
    "import h3\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "import plotly.express as px\n",
    "from shapely.wkt import loads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac523088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hexagon map\n",
    "def get_hexagon_grid(latitude, longitude, resolution, ring_size):\n",
    "    \"\"\"\n",
    "    Generate a hexagonal grid GeoDataFrame centered around a specified location.\n",
    "    Parameters:\n",
    "    - latitude (float): Latitude of the center point.\n",
    "    - longitude (float): Longitude of the center point.\n",
    "    - resolution (int): H3 resolution for hexagons.\n",
    "    - ring_size (int): Number of rings to create around the center hexagon.\n",
    "    Returns:\n",
    "    - hexagon_df (geopandas.GeoDataFrame): GeoDataFrame containing hexagons and their geometries.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the H3 hexagons covering the specified location\n",
    "    center_h3 = h3.geo_to_h3(latitude, longitude, resolution)\n",
    "    hexagons = list(h3.k_ring(center_h3, ring_size))  # Convert the set to a list\n",
    "\n",
    "    # Create a GeoDataFrame with hexagons and their corresponding geometries\n",
    "    hexagon_geometries = [shapely.geometry.Polygon(h3.h3_to_geo_boundary(hexagon, geo_json=True)) for hexagon in hexagons]\n",
    "    hexagon_df = gpd.GeoDataFrame({'Hexagon_ID': hexagons, 'geometry': hexagon_geometries})\n",
    "\n",
    "    return hexagon_df\n",
    "\n",
    "def create_choropleth_map_only_hexagon_df(geojson_df, alpha=0.1, map_style=\"carto-positron\", color_scale=\"Viridis\"):\n",
    "    \"\"\"\n",
    "    Create an interactive choropleth map using Plotly Express.\n",
    "    Parameters:\n",
    "    - geojson_df (GeoDataFrame): GeoJSON data containing polygon geometries.\n",
    "    - alpha (float): Opacity level for the map polygons (0.0 to 1.0).\n",
    "    - map_style (str): Map style for the Plotly map (e.g., \"carto-positron\").\n",
    "    - color_scale (str): Color scale for the choropleth map.\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Create a choropleth map using px.choropleth_mapbox\n",
    "    fig = px.choropleth_mapbox(\n",
    "        geojson_df,\n",
    "        geojson=geojson_df.geometry,\n",
    "        locations=geojson_df.index,  # Use index as locations to avoid duplicate rows\n",
    "        # color=\"Count\",\n",
    "        color_continuous_scale=color_scale,\n",
    "        title=\"GPS Jam Map\",\n",
    "        mapbox_style=map_style,\n",
    "        center={\"lat\": home_lat, \"lon\": home_lng},  # Adjust the center as needed\n",
    "        zoom=2,\n",
    "    )\n",
    "\n",
    "    # Customize the opacity of the hexagons\n",
    "    fig.update_traces(marker=dict(opacity=alpha))\n",
    "\n",
    "    # Set margins to 25 on all sides\n",
    "    fig.update_layout(margin=dict(l=35, r=35, t=45, b=35))\n",
    "    \n",
    "    # Adjust the width of the visualization\n",
    "    fig.update_layout(width=1000) \n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "\n",
    "# Latitude and longitude coordinates\n",
    "home_lat = 1.3472764\n",
    "home_lng = 103.9104234\n",
    "\n",
    "# Generate H3 hexagons at a specified resolution (e.g., 9)\n",
    "resolution = 5\n",
    "\n",
    "# Indicate the number of rings around the central hexagon\n",
    "ring_size = 463\n",
    "\n",
    "# Hexagon grid around HOME\n",
    "hexagon_df = get_hexagon_grid(home_lat, home_lng, resolution, ring_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b517b8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the hexagon grid is suitable\n",
    "# create_choropleth_map_only_hexagon_df(geojson_df=hexagon_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f9f1e91-4d43-4e2f-afba-774064c27fc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv exists csv\\20240601\\000000Z.csv\n",
      "csv exists csv\\20240601\\000005Z.csv\n",
      "csv exists csv\\20240601\\000010Z.csv\n",
      "csv exists csv\\20240601\\000015Z.csv\n",
      "csv exists csv\\20240601\\000020Z.csv\n",
      "csv exists csv\\20240601\\000030Z.csv\n",
      "csv exists csv\\20240601\\000025Z.csv\n",
      "csv exists csv\\20240601\\000035Z.csv\n",
      "csv exists csv\\20240601\\000040Z.csv\n",
      "csv exists csv\\20240601\\000045Z.csv\n",
      "csv exists csv\\20240601\\000055Z.csv\n",
      "csv exists csv\\20240601\\000050Z.csv\n",
      "csv exists csv\\20240601\\000100Z.csv\n",
      "2024-06-01 00:00:00\n",
      "Joined csv\\20240601\\000000Z.csv\n",
      "2024-06-01 00:00:05\n",
      "Joined csv\\20240601\\000005Z.csv\n",
      "2024-06-01 00:00:10\n",
      "Joined csv\\20240601\\000010Z.csv\n",
      "2024-06-01 00:00:15\n",
      "Joined csv\\20240601\\000015Z.csv\n",
      "2024-06-01 00:00:20\n",
      "Joined csv\\20240601\\000020Z.csv\n",
      "2024-06-01 00:00:25\n",
      "Joined csv\\20240601\\000025Z.csv\n",
      "2024-06-01 00:00:30\n",
      "Joined csv\\20240601\\000030Z.csv\n",
      "2024-06-01 00:00:35\n",
      "Joined csv\\20240601\\000035Z.csv\n",
      "2024-06-01 00:00:40\n",
      "Joined csv\\20240601\\000040Z.csv\n",
      "2024-06-01 00:00:45\n",
      "Joined csv\\20240601\\000045Z.csv\n",
      "2024-06-01 00:00:50\n",
      "Joined csv\\20240601\\000050Z.csv\n",
      "2024-06-01 00:00:55\n",
      "Joined csv\\20240601\\000055Z.csv\n",
      "2024-06-01 00:01:00\n",
      "Joined csv\\20240601\\000100Z.csv\n",
      "2024-06-01 00:01:05\n",
      "2024-06-01 00:01:10\n",
      "2024-06-01 00:01:15\n",
      "2024-06-01 00:01:20\n",
      "2024-06-01 00:01:25\n",
      "2024-06-01 00:01:30\n",
      "2024-06-01 00:01:35\n",
      "2024-06-01 00:01:40\n",
      "2024-06-01 00:01:45\n",
      "2024-06-01 00:01:50\n",
      "2024-06-01 00:01:55\n",
      "2024-06-01 00:02:00\n",
      "2024-06-01 00:02:05\n",
      "2024-06-01 00:02:10\n",
      "2024-06-01 00:02:15\n",
      "2024-06-01 00:02:20\n",
      "2024-06-01 00:02:25\n",
      "2024-06-01 00:02:30\n",
      "2024-06-01 00:02:35\n",
      "2024-06-01 00:02:40\n",
      "2024-06-01 00:02:45\n",
      "2024-06-01 00:02:50\n",
      "2024-06-01 00:02:55\n",
      "2024-06-01 00:03:00\n",
      "2024-06-01 00:03:05\n",
      "2024-06-01 00:03:10\n",
      "2024-06-01 00:03:15\n",
      "2024-06-01 00:03:20\n",
      "2024-06-01 00:03:25\n",
      "2024-06-01 00:03:30\n",
      "2024-06-01 00:03:35\n",
      "2024-06-01 00:03:40\n",
      "2024-06-01 00:03:45\n",
      "2024-06-01 00:03:50\n",
      "2024-06-01 00:03:55\n",
      "2024-06-01 00:04:00\n",
      "2024-06-01 00:04:05\n",
      "2024-06-01 00:04:10\n",
      "2024-06-01 00:04:15\n",
      "2024-06-01 00:04:20\n",
      "2024-06-01 00:04:25\n",
      "2024-06-01 00:04:30\n",
      "2024-06-01 00:04:35\n",
      "2024-06-01 00:04:40\n",
      "2024-06-01 00:04:45\n",
      "2024-06-01 00:04:50\n",
      "2024-06-01 00:04:55\n",
      "Download memory usage\n",
      "\n",
      "        Total Memory: 63.78 GB\n",
      "        Available Memory: 41.45 GB\n",
      "        Used Memory: 22.33 GB\n",
      "        Memory Usage Percentage: 35.00%\n",
      "    \n",
      "\n",
      "\n",
      "CSV memory usage\n",
      "\n",
      "Total directory size: 79.2 MB\n",
      "Download time: 0.0 min\n",
      "CSV time: 0.0 min\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>time</th>\n",
       "      <th>hexbin</th>\n",
       "      <th>hex</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>nic</th>\n",
       "      <th>type</th>\n",
       "      <th>flight</th>\n",
       "      <th>r</th>\n",
       "      <th>t</th>\n",
       "      <th>category</th>\n",
       "      <th>version</th>\n",
       "      <th>nac_p</th>\n",
       "      <th>nac_v</th>\n",
       "      <th>good_bad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20240601T075959</td>\n",
       "      <td>850ca233fffffff</td>\n",
       "      <td>c0173a</td>\n",
       "      <td>62.187831</td>\n",
       "      <td>-172.862478</td>\n",
       "      <td>8.0</td>\n",
       "      <td>adsb_icao</td>\n",
       "      <td>ACA5</td>\n",
       "      <td>C-FIUR</td>\n",
       "      <td>B77W</td>\n",
       "      <td>A3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>20240601T075959</td>\n",
       "      <td>850ca033fffffff</td>\n",
       "      <td>aaa4dc</td>\n",
       "      <td>62.821655</td>\n",
       "      <td>-171.065369</td>\n",
       "      <td>8.0</td>\n",
       "      <td>adsb_icao</td>\n",
       "      <td>UAL803</td>\n",
       "      <td>N785UA</td>\n",
       "      <td>B772</td>\n",
       "      <td>A5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>20240601T075959</td>\n",
       "      <td>850ca0affffffff</td>\n",
       "      <td>71ba03</td>\n",
       "      <td>62.915722</td>\n",
       "      <td>-170.707785</td>\n",
       "      <td>8.0</td>\n",
       "      <td>adsb_icao</td>\n",
       "      <td>KAL038</td>\n",
       "      <td>HL7203</td>\n",
       "      <td>B77W</td>\n",
       "      <td>A5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>20240601T075959</td>\n",
       "      <td>850ca0affffffff</td>\n",
       "      <td>c02ebd</td>\n",
       "      <td>62.945515</td>\n",
       "      <td>-170.615457</td>\n",
       "      <td>8.0</td>\n",
       "      <td>adsb_icao</td>\n",
       "      <td>ACA1</td>\n",
       "      <td>C-FRSE</td>\n",
       "      <td>B789</td>\n",
       "      <td>A5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>20240601T075959</td>\n",
       "      <td>850ca567fffffff</td>\n",
       "      <td>868078</td>\n",
       "      <td>63.214554</td>\n",
       "      <td>-169.761587</td>\n",
       "      <td>8.0</td>\n",
       "      <td>adsb_icao</td>\n",
       "      <td>JAL9</td>\n",
       "      <td>JA739J</td>\n",
       "      <td>B77W</td>\n",
       "      <td>A5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>20240601T075959</td>\n",
       "      <td>850c8d7bfffffff</td>\n",
       "      <td>86eb36</td>\n",
       "      <td>58.754791</td>\n",
       "      <td>-167.594575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>adsb_icao</td>\n",
       "      <td>JAL1</td>\n",
       "      <td>JA882J</td>\n",
       "      <td>B789</td>\n",
       "      <td>A5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Nan values</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>20240601T075959</td>\n",
       "      <td>850ca95bfffffff</td>\n",
       "      <td>86d9f4</td>\n",
       "      <td>60.619308</td>\n",
       "      <td>-166.267753</td>\n",
       "      <td>8.0</td>\n",
       "      <td>adsb_icao</td>\n",
       "      <td>ANA101</td>\n",
       "      <td>JA837A</td>\n",
       "      <td>B789</td>\n",
       "      <td>A5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>20240601T075959</td>\n",
       "      <td>850cad73fffffff</td>\n",
       "      <td>a4db4e</td>\n",
       "      <td>61.462966</td>\n",
       "      <td>-165.760701</td>\n",
       "      <td>9.0</td>\n",
       "      <td>adsb_icao</td>\n",
       "      <td>N4118G</td>\n",
       "      <td>N4118G</td>\n",
       "      <td>PA31</td>\n",
       "      <td>A1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11</td>\n",
       "      <td>20240601T075959</td>\n",
       "      <td>850cf60bfffffff</td>\n",
       "      <td>a31a54</td>\n",
       "      <td>59.555488</td>\n",
       "      <td>-165.468497</td>\n",
       "      <td>8.0</td>\n",
       "      <td>adsb_icao</td>\n",
       "      <td>UAL143</td>\n",
       "      <td>N29975</td>\n",
       "      <td>B789</td>\n",
       "      <td>A5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12</td>\n",
       "      <td>20240601T075959</td>\n",
       "      <td>850cad77fffffff</td>\n",
       "      <td>a4d881</td>\n",
       "      <td>61.392883</td>\n",
       "      <td>-165.38887</td>\n",
       "      <td>9.0</td>\n",
       "      <td>adsb_icao</td>\n",
       "      <td>N411GV</td>\n",
       "      <td>N411GV</td>\n",
       "      <td>C208</td>\n",
       "      <td>A1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             time           hexbin     hex        lat  \\\n",
       "0           0  20240601T075959  850ca233fffffff  c0173a  62.187831   \n",
       "1           3  20240601T075959  850ca033fffffff  aaa4dc  62.821655   \n",
       "2           4  20240601T075959  850ca0affffffff  71ba03  62.915722   \n",
       "3           5  20240601T075959  850ca0affffffff  c02ebd  62.945515   \n",
       "4           6  20240601T075959  850ca567fffffff  868078  63.214554   \n",
       "5           7  20240601T075959  850c8d7bfffffff  86eb36  58.754791   \n",
       "6           9  20240601T075959  850ca95bfffffff  86d9f4  60.619308   \n",
       "7          10  20240601T075959  850cad73fffffff  a4db4e  61.462966   \n",
       "8          11  20240601T075959  850cf60bfffffff  a31a54  59.555488   \n",
       "9          12  20240601T075959  850cad77fffffff  a4d881  61.392883   \n",
       "\n",
       "           lon  nic       type    flight       r     t category  version  \\\n",
       "0  -172.862478  8.0  adsb_icao  ACA5      C-FIUR  B77W       A3      2.0   \n",
       "1  -171.065369  8.0  adsb_icao  UAL803    N785UA  B772       A5      2.0   \n",
       "2  -170.707785  8.0  adsb_icao  KAL038    HL7203  B77W       A5      2.0   \n",
       "3  -170.615457  8.0  adsb_icao  ACA1      C-FRSE  B789       A5      2.0   \n",
       "4  -169.761587  8.0  adsb_icao  JAL9      JA739J  B77W       A5      2.0   \n",
       "5  -167.594575  0.0  adsb_icao  JAL1      JA882J  B789       A5      2.0   \n",
       "6  -166.267753  8.0  adsb_icao  ANA101    JA837A  B789       A5      2.0   \n",
       "7  -165.760701  9.0  adsb_icao  N4118G    N4118G  PA31       A1      2.0   \n",
       "8  -165.468497  8.0  adsb_icao  UAL143    N29975  B789       A5      2.0   \n",
       "9   -165.38887  9.0  adsb_icao  N411GV    N411GV  C208       A1      2.0   \n",
       "\n",
       "  nac_p       nac_v good_bad  \n",
       "0  10.0         2.0     good  \n",
       "1  10.0         2.0     good  \n",
       "2   9.0         1.0     good  \n",
       "3   9.0         2.0     good  \n",
       "4  10.0         2.0     good  \n",
       "5   9.0  Nan values      bad  \n",
       "6   9.0         1.0     good  \n",
       "7  10.0         2.0     good  \n",
       "8   9.0         2.0     good  \n",
       "9  10.0         2.0     good  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_hexagon_ids(df):\n",
    "    \"\"\"\n",
    "    Calculate Hexagon IDs for each row(ping) in a DataFrame based on their geographic coordinates.\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing ADSB data with \"lat\" and \"lon\" columns.\n",
    "        hexagon_df (gpd.GeoDataFrame): GeoDataFrame with hexagon geometries and associated Hexagon IDs.\n",
    "    Returns:\n",
    "        pd.DataFrame: The input DataFrame with an additional \"Hexagon_ID\" column indicating the Hexagon ID for each ping.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a column Hexagon_ID with the ID of the hexagon\n",
    "    df['Hexagon_ID'] = None\n",
    "\n",
    "    # Iterate through the hotels in the df DataFrame and calculate hotel counts within each hexagon\n",
    "    for i, ping in df.iterrows():\n",
    "        # point = shapely.geometry.Point(ping[\"lon\"], ping[\"lat\"])  # Latitude and Longitude switched\n",
    "        # for _, row in hexagon_df.iterrows():\n",
    "        #     # geometry = loads(row['geometry'])\n",
    "        #     if point.within(row['geometry']):\n",
    "        #         df.loc[i, 'Hexagon_ID'] = row['Hexagon_ID']\n",
    "        if not isinstance(ping['lat'], float):\n",
    "            continue  \n",
    "        resolution=5   \n",
    "        result = h3.geo_to_h3(ping[\"lat\"], ping[\"lon\"], resolution)\n",
    "        # print(f'{ping[\"lat\"]},{ping[\"lon\"]}=>{result}')\n",
    "        if result != 0:\n",
    "             df.loc[i, 'Hexagon_ID'] = result\n",
    "    \n",
    "    return df\n",
    "\n",
    "def time_elapsed(start_time):\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "time_data={\n",
    "    \"pickle\": 0,\n",
    "    \"url\": 0,\n",
    "    \"concat\": 0,\n",
    "}\n",
    "\n",
    "def get_adsb_data(data):\n",
    "    response = requests.get(data[\"url\"])\n",
    "    print(f\"downloaded {data['url']}\")\n",
    "    json_data = response.json()\n",
    "    \n",
    "    utc_time = dt.datetime.fromtimestamp(json_data['now'])\n",
    "    df = pd.json_normalize(json_data['aircraft'])\n",
    "\n",
    "    df = df[df[\"type\"] == 'adsb_icao']\n",
    "    df['time'] = utc_time.strftime('%Y%m%dT%H%M%S')\n",
    "    df['good_bad'] = df['nic'].apply(lambda x: 'good' if x >= 7 else 'bad')\n",
    "    df['Hexagon_ID'] = None\n",
    "    df = df[['time','Hexagon_ID','hex','lat','lon','nic','type', 'flight', 'r', 't', 'category', 'version', 'nac_p', 'nac_v','good_bad' ]]\n",
    "    df = df.fillna(\"Nan values\")\n",
    "    df = df.map(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "    df = calculate_hexagon_ids(df)\n",
    "\n",
    "    #save to pickle\n",
    "    if not os.path.exists(os.path.dirname(data[\"csv\"])):\n",
    "        os.makedirs(os.path.dirname(data[\"csv\"]))\n",
    "    df.to_csv(data[\"csv\"])\n",
    "    return df\n",
    "\n",
    "def download_data(index, data, length_data_array, start_time_overall):\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # if pickle exists, do nothing\n",
    "        if (os.path.isfile(data[\"csv\"])):\n",
    "            print(f\"csv exists {data['csv']}\")\n",
    "            return\n",
    "        else:\n",
    "            get_adsb_data(data)\n",
    "\n",
    "            time_data[\"url\"] += time_elapsed(start_time)\n",
    "\n",
    "            remaining_time = (time.time() - start_time_overall)/(index+1) * (length_data_array - index)\n",
    "            print(f\"Remaining execution time: {remaining_time//60:.0f}m {remaining_time%60:.0f}s\")\n",
    "\n",
    "            return\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle other exceptions\n",
    "        print(\"An error occurred:\", e)\n",
    "\n",
    "def join_large_csv(folder_path,start,end,file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "    isHeader = True\n",
    "    with open(file_path, 'w') as outfile:\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            file_datetime = datetime.strptime(file_path, 'csv\\\\%Y%m%d\\\\%H%M%SZ.csv')\n",
    "            print(file_datetime)\n",
    "            # csv_path = os.path.join('csv',start.strftime(\"%Y%m%d\"), f'{start.strftime(\"%H%M%S\")}Z.csv')\n",
    "            if filename.endswith('.csv') and start <= file_datetime and file_datetime <= end:\n",
    "                with open(file_path, 'r') as infile:\n",
    "                    # skip row header\n",
    "                    if isHeader:\n",
    "                        outfile.write(infile.read())\n",
    "                        isHeader = False\n",
    "                    else:\n",
    "                        outfile.write(''.join(infile.readlines()[1:]))\n",
    "                print(f'Joined {file_path}')\n",
    "\n",
    "def load_data(start_date_time,end_date_time):\n",
    "    delta = dt.timedelta(seconds=5)\n",
    "    start = start_date_time\n",
    "    df1 = None\n",
    "\n",
    "    data_array = []\n",
    "    while start <= end_date_time:\n",
    "        data_array.append({\n",
    "            \"url\": f'https://samples.adsbexchange.com/readsb-hist/{start.strftime(\"%Y/%m/%d\")}/{start.strftime(\"%H%M%S\")}Z.json.gz',\n",
    "            \"pickle\": os.path.join('pickle',start.strftime(\"%Y%m%d\"), f'{start.strftime(\"%H%M%S\")}Z.pkl'),\n",
    "            \"csv\": os.path.join('csv',start.strftime(\"%Y%m%d\"), f'{start.strftime(\"%H%M%S\")}Z.csv')\n",
    "            })\n",
    "        start += delta\n",
    "\n",
    "    report1=\"\"\n",
    "    report2=\"\"\n",
    "\n",
    "    start_time_overall = time.time()\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        l = [len(data_array)] * len(data_array)\n",
    "        list(executor.map(lambda i: download_data(i, data_array[i], l[i], start_time_overall), range(len(data_array))))\n",
    "        report1 = get_memory_usage()\n",
    "\n",
    "    csv_path = os.path.join('csv', f'{start.strftime(\"%Y%m%d\")}joined.csv')\n",
    "    join_large_csv(os.path.join('csv',start.strftime(\"%Y%m%d\")),start_date_time,end_date_time,csv_path)\n",
    "\n",
    "    start_time_overall = time.time()\n",
    "\n",
    "    df1 = read_csv(csv_path)\n",
    "\n",
    "    print(f\"Download memory usage\\n{report1}\\n\\n\")\n",
    "    print(f\"CSV memory usage\\n{report2}\")\n",
    "\n",
    "    # only when all the threads are complete, then concat happens\n",
    "    print(f'Total directory size: {get_directory_size(\"csv\"):.1f} MB')\n",
    "    print(f\"Download time: {(time_data['url']/3600):.1f} min\")\n",
    "    print(f\"CSV time: {(time_data['pickle']/3600):.1f} min\")\n",
    "\n",
    "    return df1\n",
    "\n",
    "\n",
    "def get_directory_size(directory):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            total_size += os.path.getsize(filepath)\n",
    "            total_size_mb = total_size / (1024 * 1024)\n",
    "    return total_size_mb\n",
    "\n",
    "def get_memory_usage():\n",
    "\n",
    "    # Get the memory usage\n",
    "    memory_usage = psutil.virtual_memory()\n",
    "\n",
    "    # Print the memory usage\n",
    "    report = f\"\"\"\n",
    "        Total Memory: {memory_usage.total / (1024 ** 3):.2f} GB\n",
    "        Available Memory: {memory_usage.available / (1024 ** 3):.2f} GB\n",
    "        Used Memory: {memory_usage.used / (1024 ** 3):.2f} GB\n",
    "        Memory Usage Percentage: {memory_usage.percent:.2f}%\n",
    "    \"\"\"\n",
    "    return report\n",
    "\n",
    "# note that for the free sample, only the first day of each month is available on the adsbexchange\n",
    "df1 = load_data(dt.datetime(2024,6,1,0,0,0),dt.datetime(2024,6,1,0,1,0))\n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "520b2547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hexbin</th>\n",
       "      <th>flight</th>\n",
       "      <th>good_count</th>\n",
       "      <th>bad_count</th>\n",
       "      <th>percentage_bad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8508ce03fffffff</td>\n",
       "      <td>AIC188</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>850b9d03fffffff</td>\n",
       "      <td>AUL574</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>850bb013fffffff</td>\n",
       "      <td>BRU8795</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>850bb01bfffffff</td>\n",
       "      <td>BRU8795</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>850c717bfffffff</td>\n",
       "      <td>N386AK</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>850c88b3fffffff</td>\n",
       "      <td>JAL1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>850c8d4ffffffff</td>\n",
       "      <td>JAL2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>850c8d7bfffffff</td>\n",
       "      <td>JAL1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>850ce27bfffffff</td>\n",
       "      <td>ANA7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>851002bbfffffff</td>\n",
       "      <td>UTA445</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              hexbin    flight  good_count  bad_count  percentage_bad\n",
       "11   8508ce03fffffff  AIC188             0          4        1.000000\n",
       "40   850b9d03fffffff  AUL574             0          5        1.000000\n",
       "44   850bb013fffffff  BRU8795            0          3        1.000000\n",
       "46   850bb01bfffffff  BRU8795            0         10        1.000000\n",
       "85   850c717bfffffff  N386AK             2          8        0.800000\n",
       "144  850c88b3fffffff  JAL1               3          1        0.250000\n",
       "145  850c8d4ffffffff  JAL2               0          1        1.000000\n",
       "146  850c8d7bfffffff  JAL1               0          5        1.000000\n",
       "168  850ce27bfffffff  ANA7               6          3        0.333333\n",
       "194  851002bbfffffff  UTA445             0          2        1.000000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df1.groupby(['Hexagon_ID', 'flight'], as_index=False).agg(good_count=('good_bad', lambda x: (x == 'good').sum()),\n",
    "                                              bad_count=('good_bad', lambda x: (x == 'bad').sum()))\n",
    "df2 = df2.assign(percentage_bad=df2['bad_count'] / (df2['good_count'] + df2['bad_count']))\n",
    "\n",
    "# df2.reset_index()\n",
    "# df2.columns = ['hexbin', 'flight', 'good_count', 'bad_count', 'percentage_bad']\n",
    "df2[df2['percentage_bad'] > 0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d690a33",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'hexbin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5028\\2468221822.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_layout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[0mcreate_choropleth_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgeojson_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhexagon_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5028\\2468221822.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(geojson_df, data_df, alpha, map_style, color_scale)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mReturns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \"\"\"\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# Merge the GeoJSON data with your DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mmerged_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgeojson_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"hexbin\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"left\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# Create a choropleth map using px.choropleth_mapbox\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     fig = px.choropleth_mapbox(\n",
      "\u001b[1;32mc:\\Users\\ernes\\Raid\\flight-data\\venv\\lib\\site-packages\\geopandas\\geodataframe.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1555\u001b[0m         \u001b[0mSee\u001b[0m \u001b[0mhttps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpydata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morg\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mpandas\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mstable\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mreference\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1557\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mdetails\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1558\u001b[0m         \"\"\"\n\u001b[1;32m-> 1559\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1560\u001b[0m         \u001b[0mgeo_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_geometry_column_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1561\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgeo_col\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1562\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGeoDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ernes\\Raid\\flight-data\\venv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m  10828\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMergeValidate\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10829\u001b[0m     ) -> DataFrame:\n\u001b[0;32m  10830\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10832\u001b[1;33m         return merge(\n\u001b[0m\u001b[0;32m  10833\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10834\u001b[0m             \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10835\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ernes\\Raid\\flight-data\\venv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         )\n\u001b[0;32m    169\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         op = _MergeOperation(\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ernes\\Raid\\flight-data\\venv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m         ) = self._get_merge_keys()\n\u001b[0m\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ernes\\Raid\\flight-data\\venv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1306\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m                         \u001b[1;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m                         \u001b[1;31m#  the latter of which will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m                         \u001b[0mlk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1310\u001b[1;33m                         \u001b[0mleft_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1311\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1312\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m                         \u001b[1;31m# work-around for merge_asof(left_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ernes\\Raid\\flight-data\\venv\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1908\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1914\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'hexbin'"
     ]
    }
   ],
   "source": [
    "def create_choropleth_map(geojson_df, data_df, alpha=0.1, map_style=\"carto-positron\", color_scale=\"Viridis\"):\n",
    "    \"\"\"\n",
    "    Create an interactive choropleth map using Plotly Express.\n",
    "    Parameters:\n",
    "    - geojson_df (GeoDataFrame): GeoJSON data containing polygon geometries.\n",
    "    - data_df (DataFrame): DataFrame containing data to be visualized on the map.\n",
    "    - alpha (float): Opacity level for the map polygons (0.0 to 1.0).\n",
    "    - map_style (str): Map style for the Plotly map (e.g., \"carto-positron\").\n",
    "    - color_scale (str): Color scale for the choropleth map.\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Merge the GeoJSON data with your DataFrame\n",
    "    merged_df = geojson_df.merge(data_df, on=\"Hexagon_ID\", how=\"left\")\n",
    "\n",
    "    # Create a choropleth map using px.choropleth_mapbox\n",
    "    fig = px.choropleth_mapbox(\n",
    "        merged_df,\n",
    "        geojson=merged_df.geometry,\n",
    "        locations=merged_df.index,  # Use index as locations to avoid duplicate rows\n",
    "        color=\"percentage_bad\",\n",
    "        color_continuous_scale=[[0, 'rgb(0,255,0)'],\n",
    "                                [0.01, 'rgb(255,255,0)'],\n",
    "                                [0.10, 'rgb(255,0,0)'],\n",
    "                                [1, 'rgb(255,0,0)']],        \n",
    "        title=\"GPS Jam Map\",\n",
    "        mapbox_style=map_style,\n",
    "        center={\"lat\": home_lat, \"lon\": home_lng},  # Adjust the center as needed\n",
    "        zoom=2,\n",
    "    )\n",
    "\n",
    "    # Customize the opacity of the hexagons\n",
    "    fig.update_traces(marker=dict(opacity=alpha))\n",
    "\n",
    "    # Add hover data for hotel names\n",
    "    # fig.update_traces(customdata=merged_df[\"Hotels\"])\n",
    "\n",
    "    # Define the hover template \n",
    "    # hover_template = \"<b>Hotels:</b> %{customdata}<extra></extra>\"\n",
    "    # fig.update_traces(hovertemplate=hover_template)\n",
    "\n",
    "    # Set margins to 25 on all sides\n",
    "    fig.update_layout(margin=dict(l=35, r=35, t=45, b=35))\n",
    "    \n",
    "    # Adjust the width of the visualization\n",
    "    fig.update_layout(width=1000) \n",
    "\n",
    "    fig.show()\n",
    "\n",
    "create_choropleth_map(geojson_df=hexagon_df, data_df=df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35defa61-e52e-4542-bc9d-6ad8e9ed37f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change in NIC/NAC/SIL indicates an anomaly, which may be due to any reason\n",
    "\n",
    "# nic: Navigation Integrity Category (2.2.3.2.7.2.6)\n",
    "# Table 1: NIC value and corresponding size of containment radius\n",
    "# NIC Containment Radius\n",
    "# 0 Unknown\n",
    "# 1 Rc < 37.04km (20nm)\n",
    "# 2 Rc < 14.816km (8nm)\n",
    "# 3 Rc < 7.408km (4nm)\n",
    "# 4 Rc < 3.704km (2nm)\n",
    "# 5 Rc < 1852m (1nm)\n",
    "# 6 Rc < 1111.2m (0.6nm)\n",
    "# Rc < 926m (0.5nm)\n",
    "# Rc < 555.6m (0.3nm)\n",
    "# 7 Rc < 370.4m (0.2nm)\n",
    "# 8 Rc < 185.2m (0.1nm)\n",
    "# 9 Rc < 75m\n",
    "# 10 Rc < 25m\n",
    "# 11 Rc < 7.5m\n",
    "\n",
    "# Assuming you have already calculated the counts\n",
    "# counts = df1['nic'].value_counts(dropna=False)\n",
    "# print(f\"List all nic values with counts {counts}\")\n",
    "# # Create bins for the histogram\n",
    "# bins = np.arange(len(counts) + 1)\n",
    "\n",
    "# # Plot the histogram with NA on the left\n",
    "# plt.hist(df1['nic'], bins=bins, align='left',rwidth=0.5)\n",
    "# plt.yscale('log')\n",
    "# plt.title('nic')\n",
    "# plt.show()\n",
    "\n",
    "print(f\"bad {df1[df1['good_bad']=='bad'].shape[0]}\")\n",
    "print(f\"total {df1.shape[0]}\")\n",
    "print(f\"% bad / total {df1[df1['good_bad']=='bad'].shape[0]/df1.shape[0]*100:.2f}%\")\n",
    "df1[df1['good_bad']=='bad'].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18977bac-e989-4d3f-83ef-07f7b2c18756",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"List all type with counts {df1['type'].value_counts()}\")\n",
    "# List all type with counts type\n",
    "# adsb_icao         9145 - messages from a Mode S or ADS-B transponder, using a 24-bit ICAO address\n",
    "# other              725 - IGNORE miscellaneous data received via Basestation / SBS format, quality / source is unknown.\n",
    "# adsb_icao_nt       524 - IGNORE - messages from an ADS-B equipped non-transponder emitter e.g. a ground vehicle, using a 24-bit ICAO address\n",
    "# mode_s             515 - ModeS data from the planes transponder (no position transmitted)\n",
    "# adsr_icao          280 - rebroadcast of ADS-B messages originally sent via another data link e.g. UAT, using a 24-bit ICAO address\n",
    "# tisb_other         256 - traffic information about a non-ADS-B target using a non-ICAO address\n",
    "# tisb_trackfile     214 - traffic information about a non-ADS-B target using a track/file identifier, typically from primary or Mode A/C radar\n",
    "# mlat               116 - MLAT, position calculated arrival time differences using multiple receivers, outliers and varying accuracy is expected.\n",
    "# unknown             49\n",
    "# tisb_icao           31 - traffic information about a non-ADS-B target identified by a 24-bit ICAO address, e.g. a Mode S target tracked by secondary radar\n",
    "# adsb_other          17 - messages from an ADS-B transponder using a non-ICAO address, e.g. anonymized address\n",
    "plt.hist(df1['type'], align='left')\n",
    "plt.title('type')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ad6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1_filtered = df1[df1[['nac_p','nac_v']].notnull().all(1)]\n",
    "nacp_high = df1_filtered[df1_filtered['nac_p']<8]\n",
    "nacp_high\n",
    "# rr_lat not null\n",
    "# rr_lon not null\n",
    "# lastPosition.lat / lastPosition.lon not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb594aeb-9bcb-4c6d-b8b9-198fe0fe03c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"List all ADSB versions with counts {df1['version'].value_counts()}\")\n",
    "# version: ADS-B Version Number 0, 1, 2 (3-7 are reserved) (2.2.3.2.7.5)\n",
    "# List all ADSB versions with counts version\n",
    "# 2.0    8881\n",
    "# 0.0    1119\n",
    "# 1.0     141\n",
    "plt.hist(df1['version'])\n",
    "plt.title('version')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f79248-50fe-4aba-8aaa-f752fdaff139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nac_p: Navigation Accuracy for Position (2.2.5.1.35)\n",
    "counts = df1['nac_p'].value_counts()\n",
    "print(f\"List all nac_p values with counts {counts}\")\n",
    "plt.hist(df1['nac_p'])\n",
    "plt.title('nac_p')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64370da9-6ac8-4082-bfc7-42f4ca487c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nac_v: Navigation Accuracy for Velocity (2.2.5.1.19)\n",
    "counts = df1['nac_v'].value_counts()\n",
    "print(f\"List all nac_v values with counts {counts}\")\n",
    "plt.hist(df1['nac_v'])\n",
    "plt.title('nac_v')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50127be1-8b8d-4b58-be77-e07329216fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roll: Roll, degrees, negative is left roll\n",
    "counts = df1['roll'].value_counts()\n",
    "print(f\"List all roll with counts {counts}\")\n",
    "plt.hist(df1['roll'], align='left')\n",
    "plt.title('roll')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fc97be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category: emitter category to identify particular aircraft or vehicle classes (values A0  D7) (2.2.3.2.5.2)\n",
    "counts = df1['category'].dropna()\n",
    "# print(f\"List all category with counts {counts}\")\n",
    "plt.hist(counts)\n",
    "plt.title('category')\n",
    "plt.show()\n",
    "# A0 : No ADS-B emitter category information. Do not use this emitter category. If no emitter category fits your installation, seek guidance from the FAA as appropriate. A1 : Light (< 15500 lbs)  Any airplane with a maximum takeoff weight less than 15,500 pounds. This includes very light aircraft (light sport aircraft) that do not meet the requirements of 14 CFR  103.1.\n",
    "# A2 : Small (15500 to 75000 lbs)  Any airplane with a maximum takeoff weight greater than or equal to15,500 pounds but less than 75,000 pounds.\n",
    "# A3 : Large (75000 to 300000 lbs)  Any airplane with a maximum takeoff weight greater than or equal to 75,000 pounds but less than 300,000 pounds that does not qualify for the high vortex category.\n",
    "# A4 :  High vortex large (aircraft such as B-757)  Any airplane with a maximum takeoff weight greater than or equal to 75,000 pounds but less than 300,000 pounds that has been determined to generate a high wake vortex. Currently, the Boeing 757 is the only example.\n",
    "# A5 : Heavy (> 300000 lbs)  Any airplane with a maximum takeoff weight equal to or above 300,000 pounds.\n",
    "# A6 : High performance (> 5g acceleration and 400 kts)  Any airplane, regardless of weight, which can maneuver in excess of 5 Gs and maintain true airspeed above 400 knots.\n",
    "# A7 : Rotorcraft  Any rotorcraft regardless of weight.\n",
    "# B0 : No ADS-B emitter category information\n",
    "# B1 : Glider / sailplane  Any glider or sailplane regardless of weight.\n",
    "# B2 : Lighter-than-air  Any lighter than air (airship or balloon) regardless of weight.\n",
    "# B3 : Parachutist / skydiver\n",
    "# B4 : Ultralight / hang-glider / paraglider  A vehicle that meets the requirements of 14 CFR  103.1. Light sport aircraft should not use the ultralight emitter category unless they meet 14 CFR  103.1.\n",
    "# B5 : Reserved\n",
    "# B6 : Unmanned aerial vehicle  Any unmanned aerial vehicle or unmanned aircraft system regardless of weight.\n",
    "# B7 : Space / trans-atmospheric vehicle\n",
    "# C0 : No ADS-B emitter category information\n",
    "# C1 : Surface vehicle  emergency vehicle\n",
    "# C2 : Surface vehicle  service vehicle\n",
    "# C3 : Point obstacle (includes tethered balloons)\n",
    "# C4 : Cluster obstacle\n",
    "# C5 : Line obstacle\n",
    "# C6 : Reserved\n",
    "# C7 : Reserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bad7bec-4363-4d1b-8ada-ae8090384b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df1['sil'].value_counts(dropna=False)\n",
    "\n",
    "print(f\"List all sil values with counts {counts}\")\n",
    "# Create bins for the histogram\n",
    "bins = np.arange(len(counts) + 1)\n",
    "\n",
    "# Plot the histogram with NA on the left\n",
    "plt.hist(df1['sil'], bins=bins, align='left',rwidth=0.5)\n",
    "plt.yscale('log')\n",
    "plt.title('sil')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b863caa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = 'https://samples.adsbexchange.com/readsb-hist/2024/02/01/000005Z.json.gz'\n",
    "response2 = requests.get(url2)\n",
    "json_data2 = response2.json()\n",
    "df2 = pd.json_normalize(json_data2['aircraft'])\n",
    "url3 = 'https://samples.adsbexchange.com/readsb-hist/2024/02/01/000010Z.json.gz'\n",
    "response3 = requests.get(url3)\n",
    "json_data3 = response2.json()\n",
    "df3 = pd.json_normalize(json_data3['aircraft'])\n",
    "url4 = 'https://samples.adsbexchange.com/readsb-hist/2024/02/01/000015Z.json.gz'\n",
    "response4 = requests.get(url4)\n",
    "json_data4 = response2.json()\n",
    "df4 = pd.json_normalize(json_data4['aircraft'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c04f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df2.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fff1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2_new = df2[(df2[\"nic\"] > 0 ) & (df2[\"nic\"] <= 14)][[\"nic\",\"hex\",*df2.columns.values]]\n",
    "df2 = df2[['hex',\n",
    " 'lat',\n",
    " 'lon',\n",
    " 'nic',\n",
    " 'type',\n",
    " 'flight',\n",
    " 'r',\n",
    " 't',\n",
    " 'category',\n",
    " 'version',\n",
    " 'nac_p',\n",
    " 'nac_v',\n",
    " 'roll',\n",
    " ]]\n",
    "df2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
