{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m venv venv\n",
    "# # !venv/scripts/activate\n",
    "%pip install pandas numpy matplotlib seaborn plotly geopandas scipy scikit-learn statsmodels requests aiohttp shapely ipython h3 ipywidgets dtale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "from pandas import read_csv\n",
    "import concurrent.futures\n",
    "import psutil\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import h3\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "import plotly.express as px\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the hexagon grid is suitable\n",
    "# create_choropleth_map_only_hexagon_df(geojson_df=hexagon_df)\n",
    "\n",
    "# def create_choropleth_map_only_hexagon_df(geojson_df, alpha=0.1, map_style=\"carto-positron\", color_scale=\"Viridis\"):\n",
    "#     \"\"\"\n",
    "#     Create an interactive choropleth map using Plotly Express.\n",
    "#     Parameters:\n",
    "#     - geojson_df (GeoDataFrame): GeoJSON data containing polygon geometries.\n",
    "#     - alpha (float): Opacity level for the map polygons (0.0 to 1.0).\n",
    "#     - map_style (str): Map style for the Plotly map (e.g., \"carto-positron\").\n",
    "#     - color_scale (str): Color scale for the choropleth map.\n",
    "#     Returns:\n",
    "#     None\n",
    "#     \"\"\"\n",
    "#     # Create a choropleth map using px.choropleth_mapbox\n",
    "#     fig = px.choropleth_mapbox(\n",
    "#         geojson_df,\n",
    "#         geojson=geojson_df.geometry,\n",
    "#         locations=geojson_df.index,  # Use index as locations to avoid duplicate rows\n",
    "#         # color=\"Count\",\n",
    "#         color_continuous_scale=color_scale,\n",
    "#         title=\"GPS Jam Map\",\n",
    "#         mapbox_style=map_style,\n",
    "#         center={\"lat\": home_lat, \"lon\": home_lng},  # Adjust the center as needed\n",
    "#         zoom=2,\n",
    "#     )\n",
    "\n",
    "#     # Customize the opacity of the hexagons\n",
    "#     fig.update_traces(marker=dict(opacity=alpha))\n",
    "\n",
    "#     # Set margins to 25 on all sides\n",
    "#     fig.update_layout(margin=dict(l=35, r=35, t=45, b=35))\n",
    "    \n",
    "#     # Adjust the width of the visualization\n",
    "#     fig.update_layout(width=1000) \n",
    "\n",
    "#     fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hexagon map\n",
    "def get_hexagon_grid(latitude, longitude, resolution, ring_size):\n",
    "    \"\"\"\n",
    "    Generate a hexagonal grid GeoDataFrame centered around a specified location.\n",
    "    Parameters:\n",
    "    - latitude (float): Latitude of the center point.\n",
    "    - longitude (float): Longitude of the center point.\n",
    "    - resolution (int): H3 resolution for hexagons.\n",
    "    - ring_size (int): Number of rings to create around the center hexagon.\n",
    "    Returns:\n",
    "    - hexagon_df (geopandas.GeoDataFrame): GeoDataFrame containing hexagons and their geometries.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the H3 hexagons covering the specified location\n",
    "    center_h3 = h3.geo_to_h3(latitude, longitude, resolution)\n",
    "    hexagons = list(h3.k_ring(center_h3, ring_size))  # Convert the set to a list\n",
    "\n",
    "    # Create a GeoDataFrame with hexagons and their corresponding geometries\n",
    "    hexagon_geometries = [shapely.geometry.Polygon(h3.h3_to_geo_boundary(hexagon, geo_json=True)) for hexagon in hexagons]\n",
    "    hexagon_df = gpd.GeoDataFrame({'Hexagon_ID': hexagons, 'geometry': hexagon_geometries})\n",
    "\n",
    "    return hexagon_df\n",
    "\n",
    "\n",
    "# Latitude and longitude coordinates\n",
    "home_lat = 1.3472764\n",
    "home_lng = 103.9104234\n",
    "\n",
    "# Generate H3 hexagons at a specified resolution (e.g., 9)\n",
    "resolution = 5\n",
    "\n",
    "# Indicate the number of rings around the central hexagon\n",
    "ring_size = 463\n",
    "\n",
    "# Hexagon grid around HOME\n",
    "hexagon_df = get_hexagon_grid(home_lat, home_lng, resolution, ring_size)\n",
    "print(\"hexagons calculated\")\n",
    "\n",
    "def calculate_hexagon_ids(df):\n",
    "    \"\"\"\n",
    "    Calculate Hexagon IDs for each row(ping) in a DataFrame based on their geographic coordinates.\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing ADSB data with \"lat\" and \"lon\" columns.\n",
    "        hexagon_df (gpd.GeoDataFrame): GeoDataFrame with hexagon geometries and associated Hexagon IDs.\n",
    "    Returns:\n",
    "        pd.DataFrame: The input DataFrame with an additional \"Hexagon_ID\" column indicating the Hexagon ID for each ping.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a column Hexagon_ID with the ID of the hexagon\n",
    "    df['Hexagon_ID'] = None\n",
    "\n",
    "    # Iterate through the hotels in the df DataFrame and calculate hotel counts within each hexagon\n",
    "    for i, ping in df.iterrows():\n",
    "        if not isinstance(ping['lat'], float):\n",
    "            continue  \n",
    "        resolution=5   \n",
    "        result = h3.geo_to_h3(ping[\"lat\"], ping[\"lon\"], resolution)\n",
    "        # print(f'{ping[\"lat\"]},{ping[\"lon\"]}=>{result}')\n",
    "        if result != 0:\n",
    "             df.loc[i, 'Hexagon_ID'] = result\n",
    "    \n",
    "    return df\n",
    "\n",
    "def time_elapsed(start_time):\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "time_data={\n",
    "    \"pickle\": 0,\n",
    "    \"url\": 0,\n",
    "    \"concat\": 0,\n",
    "}\n",
    "\n",
    "def get_adsb_data(data):\n",
    "    response = requests.get(data[\"url\"])\n",
    "    print(f\"downloaded {data['url']}\")\n",
    "    json_data = response.json()\n",
    "    \n",
    "    df = pd.json_normalize(json_data['aircraft'])\n",
    "    df = df.fillna(\"Nan values\")\n",
    "    df = df[df[\"type\"] == 'adsb_icao']\n",
    "    df['time'] = datetime.fromtimestamp(json_data['now']).strftime(\"%Y%m%d%H%M%S\")\n",
    "    df['good_bad'] = df['nic'].apply(lambda x: 'bad' if x==\"Nan values\" or int(x)<=6 else 'good')\n",
    "    df['alt_baro'] = df['alt_baro'].apply(lambda x: 0 if (x == \"ground\" or x == \"Nan values\" ) else float(x))\n",
    "    df[\"nic\"] = df['nic'].apply(lambda x: int(float(x)) if x !=\"Nan values\" else 0)\n",
    "\n",
    "\n",
    "    df['Hexagon_ID'] = None\n",
    "    df = df[['time','Hexagon_ID', 'r', 'time', 'alt_baro', 'nic', 'good_bad', 'hex','lat','lon','type','flight','t','category','version','nac_p','nac_v']]\n",
    "    df = df.map(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "    df = df[df[\"r\"]!=\"Nan values\"]\n",
    "\n",
    "    df = calculate_hexagon_ids(df)\n",
    "\n",
    "    #save to pickle\n",
    "    if not os.path.exists(os.path.dirname(data[\"csv\"])):\n",
    "        os.makedirs(os.path.dirname(data[\"csv\"]))\n",
    "    df.to_csv(data[\"csv\"])\n",
    "    return df\n",
    "\n",
    "def download_data(index, data, length_data_array, start_time_overall):\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # if pickle exists, do nothing\n",
    "        if (os.path.isfile(data[\"csv\"])):\n",
    "            print(f\"csv exists {data['csv']}\")\n",
    "            return\n",
    "        else:\n",
    "            get_adsb_data(data)\n",
    "\n",
    "            time_data[\"url\"] += time_elapsed(start_time)\n",
    "\n",
    "            remaining_time = (time.time() - start_time_overall)/(index+1) * (length_data_array - index)\n",
    "            print(f\"Remaining execution time: {remaining_time//60:.0f}m {remaining_time%60:.0f}s\")\n",
    "\n",
    "            return\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle other exceptions\n",
    "        print(\"An error occurred:\", e)\n",
    "\n",
    "def join_large_csv(folder_path,start,end,file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "    isHeader = True\n",
    "    with open(file_path, 'w') as outfile:\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            file_datetime = datetime.strptime(file_path, 'csv\\\\%Y%m%d\\\\%H%M%SZ.csv')\n",
    "            print(file_datetime)\n",
    "            # csv_path = os.path.join('csv',start.strftime(\"%Y%m%d\"), f'{start.strftime(\"%H%M%S\")}Z.csv')\n",
    "            if filename.endswith('.csv') and start <= file_datetime and file_datetime <= end:\n",
    "                with open(file_path, 'r') as infile:\n",
    "                    # skip row header\n",
    "                    if isHeader:\n",
    "                        outfile.write(infile.read())\n",
    "                        isHeader = False\n",
    "                    else:\n",
    "                        outfile.write(''.join(infile.readlines()[1:]))\n",
    "                print(f'Joined {file_path}')\n",
    "\n",
    "def sampling(datestring, file_path):\n",
    "    df1 = read_csv(file_path)\n",
    "    df1_bad = df1[df1['good_bad']=='bad']\n",
    "    df1_bad.to_csv(os.path.join('csv','joined', f'{datestring}bad.csv'))\n",
    "\n",
    "    df1_good = df1[df1['good_bad']=='good']\n",
    "    df1_good.to_csv(os.path.join('csv','joined', f'{datestring}good.csv'))\n",
    "\n",
    "    sampled_file_path = os.path.join('csv','joined', f'{datestring}sampled.csv')\n",
    "    bad_path = os.path.join('csv','joined', f'{datestring}bad.csv')\n",
    "    good_path = os.path.join('csv','joined', f'{datestring}good.csv')\n",
    "    with open(sampled_file_path, 'w') as outfile:\n",
    "        with open(bad_path, 'r') as infile:\n",
    "            outfile.write(infile.read())\n",
    "        with open(good_path, 'r') as infile:\n",
    "            lines = infile.readlines()\n",
    "            filtered_lines = [line for i, line in enumerate(lines[1:]) if (i + 1) % 10 != 0]\n",
    "            outfile.write(''.join(filtered_lines))\n",
    "        print(f'Joined {file_path}')\n",
    "    df1 = read_csv(sampled_file_path)\n",
    "\n",
    "    df1 = df1.sort_values(by=[\"Hexagon_ID\",\"r\",\"time\"], ascending=[True,True,True])\n",
    "    df1.to_csv(sampled_file_path)\n",
    "\n",
    "def load_data(start_date_time,end_date_time, reload=False):\n",
    "    delta = dt.timedelta(seconds=5)\n",
    "    start = start_date_time\n",
    "    df1 = None\n",
    "    df2 = None\n",
    "    csv_path = os.path.join('csv', 'joined', f'{start_date_time.strftime(\"%Y%m%d\")}joined.csv')\n",
    "    hexbin_path = os.path.join('csv', 'hexbin', f'{start_date_time.strftime(\"%Y%m%d\")}hexbin.csv')\n",
    "    sampled_path = os.path.join('csv', 'joined', f'{start_date_time.strftime(\"%Y%m%d\")}sampled.csv')\n",
    "\n",
    "    if not reload[0]:\n",
    "\n",
    "        data_array = []\n",
    "        while start < end_date_time:\n",
    "            data_array.append({\n",
    "                \"url\": f'https://samples.adsbexchange.com/readsb-hist/{start.strftime(\"%Y/%m/%d\")}/{start.strftime(\"%H%M%S\")}Z.json.gz',\n",
    "                \"pickle\": os.path.join('pickle',start.strftime(\"%Y%m%d\"), f'{start.strftime(\"%H%M%S\")}Z.pkl'),\n",
    "                \"csv\": os.path.join('csv',start.strftime(\"%Y%m%d\"), f'{start.strftime(\"%H%M%S\")}Z.csv')\n",
    "                })\n",
    "            start += delta\n",
    "\n",
    "        report1=\"\"\n",
    "        report2=\"\"\n",
    "\n",
    "        start_time_overall = time.time()\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            l = [len(data_array)] * len(data_array)\n",
    "            list(executor.map(lambda i: download_data(i, data_array[i], l[i], start_time_overall), range(len(data_array))))\n",
    "            report1 = get_memory_usage()\n",
    "\n",
    "        join_large_csv(os.path.join('csv',start_date_time.strftime(\"%Y%m%d\")),start_date_time,end_date_time,csv_path)\n",
    "        sampling(start_date_time.strftime(\"%Y%m%d\"), csv_path)\n",
    "\n",
    "        start_time_overall = time.time()\n",
    "        print(f\"Download memory usage\\n{report1}\\n\\n\")\n",
    "        print(f\"CSV memory usage\\n{report2}\")\n",
    "        # only when all the threads are complete, then concat happens\n",
    "        print(f'Total directory size: {get_directory_size(\"csv\"):.1f} MB')\n",
    "        print(f\"Download time: {(time_data['url']/3600):.1f} min\")\n",
    "        print(f\"CSV time: {(time_data['pickle']/3600):.1f} min\")\n",
    "    \n",
    "    print(\"loading csv\")\n",
    "    df1 = read_csv(sampled_path)\n",
    "    print(\"csv loaded\")\n",
    "\n",
    "    if not reload[1]:\n",
    "\n",
    "        df2 = df1.groupby(['Hexagon_ID', 'r'], as_index=False).agg(\n",
    "                            good_count=('good_bad', lambda x: (x == 'good').sum()),\n",
    "                            bad_count=('good_bad', lambda x: (x == 'bad').sum()),\n",
    "                            alt_baro_range=('alt_baro', lambda x: (x.max() - x.min())),\n",
    "                            time_range=('time', lambda x: x.max() - x.min()))\n",
    "\n",
    "        df2[\"total_count\"]=df2[\"good_count\"]+df2[\"bad_count\"]\n",
    "        df2[\"percentage_bad\"]=df2[\"bad_count\"]/df2[\"total_count\"]\n",
    "        def get_lat(row):\n",
    "            return h3.h3_to_geo(row['Hexagon_ID'])[0]\n",
    "\n",
    "        def get_lng(row):\n",
    "            return h3.h3_to_geo(row['Hexagon_ID'])[1]\n",
    "\n",
    "        # Apply the function to create new 'lat' and 'lng' columns\n",
    "        df2['lat'] = df2.apply(get_lat, axis=1)\n",
    "        df2['lng'] = df2.apply(get_lng, axis=1)\n",
    "\n",
    "        unique = df1.groupby('Hexagon_ID', as_index=False)['r'].nunique().sort_values('r',ascending=False)\n",
    "        unique = unique.rename(columns={\"r\": \"r_count\"})\n",
    "\n",
    "        df2 = df2.merge(unique, on='Hexagon_ID', how='left').sort_values([\"r_count\", \"Hexagon_ID\"], ascending=[False,True])\n",
    "        df2 = df2[['Hexagon_ID', 'r_count',\t'r', 'good_count','bad_count','alt_baro_range','time_range','total_count','percentage_bad']]\n",
    "        df2 = df2.groupby('Hexagon_ID')[['r_count','r', 'good_count','bad_count','alt_baro_range','time_range','total_count','percentage_bad']]\n",
    "        df2.sum().reset_index().to_csv(hexbin_path)\n",
    "    \n",
    "    else:\n",
    "        df2 = read_csv(hexbin_path)\n",
    "\n",
    "    return [df1, df2]\n",
    "\n",
    "\n",
    "def get_directory_size(directory):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            total_size += os.path.getsize(filepath)\n",
    "            total_size_mb = total_size / (1024 * 1024)\n",
    "    return total_size_mb\n",
    "\n",
    "def get_memory_usage():\n",
    "\n",
    "    # Get the memory usage\n",
    "    memory_usage = psutil.virtual_memory()\n",
    "\n",
    "    # Print the memory usage\n",
    "    report = f\"\"\"\n",
    "        Total Memory: {memory_usage.total / (1024 ** 3):.2f} GB\n",
    "        Available Memory: {memory_usage.available / (1024 ** 3):.2f} GB\n",
    "        Used Memory: {memory_usage.used / (1024 ** 3):.2f} GB\n",
    "        Memory Usage Percentage: {memory_usage.percent:.2f}%\n",
    "    \"\"\"\n",
    "    return report\n",
    "\n",
    "# note that for the free sample, only the first day of each month is available on the adsbexchange\n",
    "# 4 hours + to download 1 day of files\n",
    "# 18 mins to reload\n",
    "\n",
    "[df1,df2] = load_data(dt.datetime(2024,6,1,0,0,0),dt.datetime(2024,6,2,0,0,0), reload=[True, False])\n",
    "\n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split the original csv into good and bad. For the bad load all lines. For the good load only samples\n",
    "\n",
    "# datestring = \"20240601\"\n",
    "# df1 = read_csv(os.path.join('csv','joined', f'{datestring}joined.csv'))\n",
    "# df1[\"nic\"] = df1['nic'].apply(lambda x: int(float(x)) if x !=\"Nan values\" else 0)\n",
    "# df1['good_bad'] = df1['nic'].apply(lambda x: 'bad' if x==\"Nan values\" or int(x)<=6 else 'good')\n",
    "# df1['alt_baro'] = df1['alt_baro'].apply(lambda x: 0 if (x == \"ground\" or x == \"Nan values\" ) else int(x))\n",
    "\n",
    "# df1=df1[['Hexagon_ID', 'r', 'time', 'alt_baro', 'nic', 'good_bad', 'hex','lat','lon','type','flight','t','category','version','nac_p','nac_v']]\n",
    "# df1 = df1.sort_values(by=[\"Hexagon_ID\",\"r\",\"time\"], ascending=[True,True,True])\n",
    "\n",
    "# df1_bad = df1[df1['good_bad']=='bad']\n",
    "# df1_bad.to_csv(os.path.join('csv','joined', f'{datestring}bad.csv'))\n",
    "\n",
    "# df1_good = df1[df1['good_bad']=='good']\n",
    "# df1_good.to_csv(os.path.join('csv','joined', f'{datestring}good.csv'))\n",
    "\n",
    "# print(df1_bad.head(10))\n",
    "# print(df1_good.head(10))\n",
    "\n",
    "# # %%\n",
    "# file_path = os.path.join('csv','joined', f'{datestring}joined-2.csv')\n",
    "# bad_path = os.path.join('csv','joined', f'{datestring}bad.csv')\n",
    "# good_path = os.path.join('csv','joined', f'{datestring}good.csv')\n",
    "# with open(file_path, 'w') as outfile:\n",
    "#     with open(bad_path, 'r') as infile:\n",
    "#         outfile.write(infile.read())\n",
    "#         isHeader = False\n",
    "#     with open(good_path, 'r') as infile:\n",
    "#         lines = infile.readlines()\n",
    "#         filtered_lines = [line for i, line in enumerate(lines[1:]) if (i + 1) % 10 != 0]\n",
    "#         outfile.write(''.join(filtered_lines))\n",
    "#     print(f'Joined {file_path}')\n",
    "# df1 = read_csv(os.path.join('csv','joined', f'{datestring}joined-2.csv'))\n",
    "# df1 = df1[df1[\"r\"]!=\"Nan values\"]\n",
    "\n",
    "# df1 = df1.sort_values(by=[\"Hexagon_ID\",\"r\",\"time\"], ascending=[True,True,True])\n",
    "# df1.to_csv(os.path.join('csv','joined', f'{datestring}joined-2.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df1.groupby(['Hexagon_ID', 'r'], as_index=False).agg(\n",
    "#     good_count=('good_bad', lambda x: (x == 'good').sum()),\n",
    "#     bad_count=('good_bad', lambda x: (x == 'bad').sum()),\n",
    "#     alt_baro_range=('alt_baro', lambda x: (x.max() - x.min())),\n",
    "#     time_range=('time', lambda x: x.max() - x.min()))\n",
    "# df2[\"total_count\"]=df2[\"good_count\"]+df2[\"bad_count\"]\n",
    "# df2[\"percentage_bad\"]=df2[\"bad_count\"]/df2[\"total_count\"]\n",
    "# unique = df1.groupby('Hexagon_ID', as_index=False)['r'].nunique().sort_values('r',ascending=False)\n",
    "# unique = unique.rename(columns={\"r\": \"r_count\"})\n",
    "\n",
    "# print(unique.head(10))\n",
    "\n",
    "# df2 = df2.merge(unique, on='Hexagon_ID', how='left').sort_values([\"r_count\", \"Hexagon_ID\"], ascending=[False,True])\n",
    "# df2 = df2[['Hexagon_ID', 'r_count',\t'r', 'good_count','bad_count','alt_baro_range','time_range','total_count','percentage_bad']]\n",
    "# df2.to_csv(os.path.join('csv','hexbin', f'{datestring}hexbin.csv'))\n",
    "\n",
    "# df3 = df2.groupby('Hexagon_ID')[['r_count','r', 'good_count','bad_count','alt_baro_range','time_range','total_count','percentage_bad']]\n",
    "\n",
    "# df3.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select these rows because it has the highest confidence\n",
    "# df2_select = df2.query('bad_count>0 and total_count>20 and percentage_bad<1')\n",
    "# print(f\"{df2_select.shape[0]} of {df2.shape[0]}\")\n",
    "# df2_select = df2_select.sort_values([\"bad_count\"], ascending=[False])\n",
    "\n",
    "# df2_select.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://towardsdatascience.com/3-python-packages-for-interactive-data-analysis-3063a201a589\n",
    "# https://towardsdatascience.com/4-libraries-that-can-perform-eda-in-one-line-of-python-code-b13938a06ae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "ax1.plot(df1['time'], df1['alt_baro'], 'b-', label='alt_baro')\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('Altitude (ft)', color='b')\n",
    "ax1.tick_params('y', colors='b')\n",
    "ax1.set_yticks(np.arange(0, 50000, 5000))\n",
    "start_time = datetime.timestamp(dt.datetime(2024,6,1,0,0,0))\n",
    "end_time = datetime.timestamp(dt.datetime(2024,6,2,0,0,0))\n",
    "xticks = np.arange(start_time, end_time, 3600)\n",
    "xticksminor=np.arange(start_time, end_time, 900)\n",
    "xticksminor=xticksminor[xticksminor//3600!=0]\n",
    "# for i in np.arange(start_time, end_time, 900)]\n",
    "xlabels = [f\"{datetime.strftime(datetime.fromtimestamp(x), '%Y-%m-%d %H:%M:%S')}\" for x in xticks]\n",
    "xlabelsminor = [f\"{datetime.strftime(datetime.fromtimestamp(x), '%H:%M:%S')}\" for x in xticksminor]\n",
    "\n",
    "ax1.set_xticks(xticks, labels=xlabels, rotation=90, fontsize=10)\n",
    "ax1.set_xticks(xticksminor, labels=xlabelsminor, rotation=90, fontsize=10, minor=True)\n",
    "ax1.tick_params('x', labelbottom='True')\n",
    "plt.xlim((start_time, end_time))\n",
    "\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(df1['time'], df1['nic'], 'rx', label='nic')\n",
    "ax2.set_ylabel('NIC', color='g')\n",
    "ax2.tick_params('y', colors='g')\n",
    "ax2.set_yticks(np.arange(0, 14, 1))\n",
    "\n",
    "\n",
    "plt.title('Altitude and NIC vs Time')\n",
    "fig.legend(loc=\"upper right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_choropleth_map(geojson_df, data_df, alpha=0.3, map_style=\"carto-positron\", data=\"percentage_bad\", limits=[0,0.1,0.5,1]):\n",
    "    \"\"\"\n",
    "    Create an interactive choropleth map using Plotly Express.\n",
    "    Parameters:\n",
    "    - geojson_df (GeoDataFrame): GeoJSON data containing polygon geometries.\n",
    "    - data_df (DataFrame): DataFrame containing data to be visualized on the map.\n",
    "    - alpha (float): Opacity level for the map polygons (0.0 to 1.0).\n",
    "    - map_style (str): Map style for the Plotly map (e.g., \"carto-positron\").\n",
    "    - color_scale (str): Color scale for the choropleth map.\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Merge the GeoJSON data with your DataFrame\n",
    "    merged_df = geojson_df.merge(data_df, on=\"Hexagon_ID\", how=\"left\")\n",
    "\n",
    "    # Create a choropleth map using px.choropleth_mapbox\n",
    "    fig = px.choropleth_mapbox(\n",
    "        merged_df,\n",
    "        geojson=merged_df.geometry,\n",
    "        locations=merged_df.index,  # Use index as locations to avoid duplicate rows\n",
    "        color=data,\n",
    "        color_continuous_scale=[[limits[0], f'rgba(0,255,0,{alpha})'],\n",
    "                                [limits[1], f'rgba(255,255,0,{alpha})'],\n",
    "                                [limits[2], f'rgba(255,0,0,{alpha})'],\n",
    "                                [limits[3], f'rgba(255,0,0,{alpha})']],        \n",
    "        title=\"GPS Jam Map\",\n",
    "        mapbox_style=map_style,\n",
    "        center={\"lat\": home_lat, \"lon\": home_lng},  # Adjust the center as needed\n",
    "        zoom=2,\n",
    "    )\n",
    "\n",
    "    # Customize the opacity of the hexagons\n",
    "    fig.update_traces(marker=dict(opacity=alpha))\n",
    "\n",
    "    # Add hover data for hotel names\n",
    "    fig.update_traces(customdata=merged_df[[\"Hexagon_ID\",\"bad_count\", \"total_count\", \"percentage_bad\", \"lat\", \"lng\"]])\n",
    "\n",
    "    # Define the hover template \n",
    "    hover_template = \"<b>Hexagon ID:</b> %{customdata[0]}<br><b>Location:</b> %{customdata[4]:.4f},%{customdata[5]:.4f}<br><b>Percentage bad:</b> %{customdata[3]:.3f}<br><b>Total Count:</b> %{customdata[2]}<extra></extra>\"\n",
    "    fig.update_traces(hovertemplate=hover_template)\n",
    "\n",
    "    # Set margins to 25 on all sides\n",
    "    fig.update_layout(margin=dict(l=35, r=35, t=45, b=35))\n",
    "    \n",
    "    # Adjust the width of the visualization\n",
    "    fig.update_layout(width=1000) \n",
    "\n",
    "    fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "create_choropleth_map(geojson_df=hexagon_df, data_df=df2, data=\"percentage_bad\", limits = [0,0.1,0.5,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "create_choropleth_map(geojson_df=hexagon_df, data_df=df2, data=\"bad_count\", limits = [0,20,50,100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Change in NIC/NAC/SIL indicates an anomaly, which may be due to any reason\n",
    "\n",
    "# nic: Navigation Integrity Category (2.2.3.2.7.2.6)\n",
    "# Table 1: NIC value and corresponding size of containment radius\n",
    "# NIC Containment Radius\n",
    "# 0 Unknown\n",
    "# 1 Rc < 37.04km (20nm)\n",
    "# 2 Rc < 14.816km (8nm)\n",
    "# 3 Rc < 7.408km (4nm)\n",
    "# 4 Rc < 3.704km (2nm)\n",
    "# 5 Rc < 1852m (1nm)\n",
    "# 6 Rc < 1111.2m (0.6nm)\n",
    "# Rc < 926m (0.5nm)\n",
    "# Rc < 555.6m (0.3nm)\n",
    "# 7 Rc < 370.4m (0.2nm)\n",
    "# 8 Rc < 185.2m (0.1nm)\n",
    "# 9 Rc < 75m\n",
    "# 10 Rc < 25m\n",
    "# 11 Rc < 7.5m\n",
    "\n",
    "# Assuming you have already calculated the counts\n",
    "# counts = df1['nic'].value_counts(dropna=False)\n",
    "# print(f\"List all nic values with counts {counts}\")\n",
    "# # Create bins for the histogram\n",
    "# bins = np.arange(len(counts) + 1)\n",
    "\n",
    "# # Plot the histogram with NA on the left\n",
    "# plt.hist(df1['nic'], bins=bins, align='left',rwidth=0.5)\n",
    "# plt.yscale('log')\n",
    "# plt.title('nic')\n",
    "# plt.show()\n",
    "\n",
    "print(f\"bad {df1[df1['good_bad']=='bad'].shape[0]}\")\n",
    "print(f\"total {df1.shape[0]}\")\n",
    "print(f\"% bad / total {df1[df1['good_bad']=='bad'].shape[0]/df1.shape[0]*100:.2f}%\")\n",
    "df1[df1['good_bad']=='bad'].head(100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "print(f\"List all type with counts {df1['type'].value_counts()}\")\n",
    "# List all type with counts type\n",
    "# adsb_icao         9145 - messages from a Mode S or ADS-B transponder, using a 24-bit ICAO address\n",
    "# other              725 - IGNORE miscellaneous data received via Basestation / SBS format, quality / source is unknown.\n",
    "# adsb_icao_nt       524 - IGNORE - messages from an ADS-B equipped “non-transponder” emitter e.g. a ground vehicle, using a 24-bit ICAO address\n",
    "# mode_s             515 - ModeS data from the planes transponder (no position transmitted)\n",
    "# adsr_icao          280 - rebroadcast of ADS-B messages originally sent via another data link e.g. UAT, using a 24-bit ICAO address\n",
    "# tisb_other         256 - traffic information about a non-ADS-B target using a non-ICAO address\n",
    "# tisb_trackfile     214 - traffic information about a non-ADS-B target using a track/file identifier, typically from primary or Mode A/C radar\n",
    "# mlat               116 - MLAT, position calculated arrival time differences using multiple receivers, outliers and varying accuracy is expected.\n",
    "# unknown             49\n",
    "# tisb_icao           31 - traffic information about a non-ADS-B target identified by a 24-bit ICAO address, e.g. a Mode S target tracked by secondary radar\n",
    "# adsb_other          17 - messages from an ADS-B transponder using a non-ICAO address, e.g. anonymized address\n",
    "plt.hist(df1['type'], align='left')\n",
    "plt.title('type')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "\n",
    "df1_filtered = df1[df1[['nac_p','nac_v']].notnull().all(1)]\n",
    "nacp_high = df1_filtered[df1_filtered['nac_p']<8]\n",
    "nacp_high\n",
    "# rr_lat not null\n",
    "# rr_lon not null\n",
    "# lastPosition.lat / lastPosition.lon not null\n",
    "\n",
    "# %%\n",
    "print(f\"List all ADSB versions with counts {df1['version'].value_counts()}\")\n",
    "# version: ADS-B Version Number 0, 1, 2 (3-7 are reserved) (2.2.3.2.7.5)\n",
    "# List all ADSB versions with counts version\n",
    "# 2.0    8881\n",
    "# 0.0    1119\n",
    "# 1.0     141\n",
    "plt.hist(df1['version'])\n",
    "plt.title('version')\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# nac_p: Navigation Accuracy for Position (2.2.5.1.35)\n",
    "counts = df1['nac_p'].value_counts()\n",
    "print(f\"List all nac_p values with counts {counts}\")\n",
    "plt.hist(df1['nac_p'])\n",
    "plt.title('nac_p')\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# nac_v: Navigation Accuracy for Velocity (2.2.5.1.19)\n",
    "counts = df1['nac_v'].value_counts()\n",
    "print(f\"List all nac_v values with counts {counts}\")\n",
    "plt.hist(df1['nac_v'])\n",
    "plt.title('nac_v')\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# roll: Roll, degrees, negative is left roll\n",
    "counts = df1['roll'].value_counts()\n",
    "print(f\"List all roll with counts {counts}\")\n",
    "plt.hist(df1['roll'], align='left')\n",
    "plt.title('roll')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# category: emitter category to identify particular aircraft or vehicle classes (values A0 – D7) (2.2.3.2.5.2)\n",
    "counts = df1['category'].dropna()\n",
    "# print(f\"List all category with counts {counts}\")\n",
    "plt.hist(counts)\n",
    "plt.title('category')\n",
    "plt.show()\n",
    "# A0 : No ADS-B emitter category information. Do not use this emitter category. If no emitter category fits your installation, seek guidance from the FAA as appropriate. A1 : Light (< 15500 lbs) – Any airplane with a maximum takeoff weight less than 15,500 pounds. This includes very light aircraft (light sport aircraft) that do not meet the requirements of 14 CFR § 103.1.\n",
    "# A2 : Small (15500 to 75000 lbs) – Any airplane with a maximum takeoff weight greater than or equal to15,500 pounds but less than 75,000 pounds.\n",
    "# A3 : Large (75000 to 300000 lbs) – Any airplane with a maximum takeoff weight greater than or equal to 75,000 pounds but less than 300,000 pounds that does not qualify for the high vortex category.\n",
    "# A4 :  High vortex large (aircraft such as B-757) – Any airplane with a maximum takeoff weight greater than or equal to 75,000 pounds but less than 300,000 pounds that has been determined to generate a high wake vortex. Currently, the Boeing 757 is the only example.\n",
    "# A5 : Heavy (> 300000 lbs) – Any airplane with a maximum takeoff weight equal to or above 300,000 pounds.\n",
    "# A6 : High performance (> 5g acceleration and 400 kts) – Any airplane, regardless of weight, which can maneuver in excess of 5 G’s and maintain true airspeed above 400 knots.\n",
    "# A7 : Rotorcraft – Any rotorcraft regardless of weight.\n",
    "# B0 : No ADS-B emitter category information\n",
    "# B1 : Glider / sailplane – Any glider or sailplane regardless of weight.\n",
    "# B2 : Lighter-than-air – Any lighter than air (airship or balloon) regardless of weight.\n",
    "# B3 : Parachutist / skydiver\n",
    "# B4 : Ultralight / hang-glider / paraglider – A vehicle that meets the requirements of 14 CFR § 103.1. Light sport aircraft should not use the ultralight emitter category unless they meet 14 CFR § 103.1.\n",
    "# B5 : Reserved\n",
    "# B6 : Unmanned aerial vehicle – Any unmanned aerial vehicle or unmanned aircraft system regardless of weight.\n",
    "# B7 : Space / trans-atmospheric vehicle\n",
    "# C0 : No ADS-B emitter category information\n",
    "# C1 : Surface vehicle – emergency vehicle\n",
    "# C2 : Surface vehicle – service vehicle\n",
    "# C3 : Point obstacle (includes tethered balloons)\n",
    "# C4 : Cluster obstacle\n",
    "# C5 : Line obstacle\n",
    "# C6 : Reserved\n",
    "# C7 : Reserved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "counts = df1['sil'].value_counts(dropna=False)\n",
    "\n",
    "print(f\"List all sil values with counts {counts}\")\n",
    "# Create bins for the histogram\n",
    "bins = np.arange(len(counts) + 1)\n",
    "\n",
    "# Plot the histogram with NA on the left\n",
    "plt.hist(df1['sil'], bins=bins, align='left',rwidth=0.5)\n",
    "plt.yscale('log')\n",
    "plt.title('sil')\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "url2 = 'https://samples.adsbexchange.com/readsb-hist/2024/02/01/000005Z.json.gz'\n",
    "response2 = requests.get(url2)\n",
    "json_data2 = response2.json()\n",
    "df2 = pd.json_normalize(json_data2['aircraft'])\n",
    "url3 = 'https://samples.adsbexchange.com/readsb-hist/2024/02/01/000010Z.json.gz'\n",
    "response3 = requests.get(url3)\n",
    "json_data3 = response2.json()\n",
    "df3 = pd.json_normalize(json_data3['aircraft'])\n",
    "url4 = 'https://samples.adsbexchange.com/readsb-hist/2024/02/01/000015Z.json.gz'\n",
    "response4 = requests.get(url4)\n",
    "json_data4 = response2.json()\n",
    "df4 = pd.json_normalize(json_data4['aircraft'])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
