{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m venv venv\n",
    "# # !venv/scripts/activate\n",
    "%pip install pandas numpy matplotlib seaborn plotly geopandas scipy scikit-learn statsmodels requests aiohttp shapely ipython h3 ipywidgets dtale tqdm pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "from pandas import read_csv\n",
    "import concurrent.futures\n",
    "import psutil\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import h3\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "import plotly.express as px\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from pandas import read_pickle\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import seaborn as sn\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the hexagon grid is suitable\n",
    "# create_choropleth_map_only_hexagon_df(geojson_df=hexagon_df)\n",
    "\n",
    "# def create_choropleth_map_only_hexagon_df(geojson_df, alpha=0.1, map_style=\"carto-positron\", color_scale=\"Viridis\"):\n",
    "#     \"\"\"\n",
    "#     Create an interactive choropleth map using Plotly Express.\n",
    "#     Parameters:\n",
    "#     - geojson_df (GeoDataFrame): GeoJSON data containing polygon geometries.\n",
    "#     - alpha (float): Opacity level for the map polygons (0.0 to 1.0).\n",
    "#     - map_style (str): Map style for the Plotly map (e.g., \"carto-positron\").\n",
    "#     - color_scale (str): Color scale for the choropleth map.\n",
    "#     Returns:\n",
    "#     None\n",
    "#     \"\"\"\n",
    "#     # Create a choropleth map using px.choropleth_mapbox\n",
    "#     fig = px.choropleth_mapbox(\n",
    "#         geojson_df,\n",
    "#         geojson=geojson_df.geometry,\n",
    "#         locations=geojson_df.index,  # Use index as locations to avoid duplicate rows\n",
    "#         # color=\"Count\",\n",
    "#         color_continuous_scale=color_scale,\n",
    "#         title=\"GPS Jam Map\",\n",
    "#         mapbox_style=map_style,\n",
    "#         center={\"lat\": home_lat, \"lon\": home_lng},  # Adjust the center as needed\n",
    "#         zoom=2,\n",
    "#     )\n",
    "\n",
    "#     # Customize the opacity of the hexagons\n",
    "#     fig.update_traces(marker=dict(opacity=alpha))\n",
    "\n",
    "#     # Set margins to 25 on all sides\n",
    "#     fig.update_layout(margin=dict(l=35, r=35, t=45, b=35))\n",
    "    \n",
    "#     # Adjust the width of the visualization\n",
    "#     fig.update_layout(width=1000) \n",
    "\n",
    "#     fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting csv to parquet\n",
      "Conversion progress: 158   \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Latitude and longitude coordinates\n",
    "home_lat = 1.3472764\n",
    "home_lng = 103.9104234\n",
    "\n",
    "# Generate H3 hexagons at a specified resolution (e.g., 9)\n",
    "resolution = 5\n",
    "\n",
    "# Indicate the number of rings around the central hexagon\n",
    "ring_size = 463\n",
    "\n",
    "# global dataframes to avoid keep loading files\n",
    "df1=None\n",
    "df2=None\n",
    "flights_data=None\n",
    "hexagon_df=None\n",
    "\n",
    "def calculate_hexagon_ids(df):\n",
    "    \"\"\"\n",
    "    Calculate Hexagon IDs for each row(ping) in a DataFrame based on their geographic coordinates.\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing ADSB data with \"lat\" and \"lon\" columns.\n",
    "        hexagon_df (gpd.GeoDataFrame): GeoDataFrame with hexagon geometries and associated Hexagon IDs.\n",
    "    Returns:\n",
    "        pd.DataFrame: The input DataFrame with an additional \"Hexagon_ID\" column indicating the Hexagon ID for each ping.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a column Hexagon_ID with the ID of the hexagon\n",
    "    df['Hexagon_ID'] = None\n",
    "\n",
    "    # Iterate through the hotels in the df DataFrame and calculate hotel counts within each hexagon\n",
    "    for i, ping in df.iterrows():\n",
    "        if not isinstance(ping['lat'], float):\n",
    "            # use location of last ping\n",
    "            found = False\n",
    "            j=1\n",
    "            while not found:\n",
    "                if isinstance(df.loc[i-j,'lat'],float):\n",
    "                    df.loc[i,'lat']=df.loc[i-j,'lat']\n",
    "                    df.loc[i,'lng']=df.loc[i-j,'lng']\n",
    "                elif isinstance(df.loc[i+j,'lat'],float):\n",
    "                    df.loc[i,'lat']=df.loc[i+j,'lat']\n",
    "                    df.loc[i,'lng']=df.loc[i+j,'lng']\n",
    "                else:\n",
    "                    j+=1\n",
    "            continue\n",
    "        resolution=5   \n",
    "        result = h3.geo_to_h3(ping[\"lat\"], ping[\"lon\"], resolution)\n",
    "        # print(f'{ping[\"lat\"]},{ping[\"lon\"]}=>{result}')\n",
    "        if result != 0:\n",
    "             df.loc[i, 'Hexagon_ID'] = result\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_adsb_data(data):\n",
    "    try:\n",
    "        response = requests.get(data[\"url\"])\n",
    "        print(f\"downloaded {data['url']}\")\n",
    "        json_data = response.json()\n",
    "        \n",
    "        df = pd.json_normalize(json_data['aircraft'])\n",
    "\n",
    "        df = df[df[\"type\"] == 'adsb_icao']\n",
    "        df['time'] = pd.to_datetime(datetime.fromtimestamp(json_data['now']).strftime(\"%Y%m%d%H%M%S\"))\n",
    "        df['Hexagon_ID'] = pd.NA\n",
    "        df['good_bad'] = np.NaN\n",
    "        df = df[['flight','r','time', 'hex', 'Hexagon_ID', 'alt_baro', 'nic', 'good_bad','lat','lon','type','t','category','version','nac_p','nac_v','track','baro_rate','seen_pos','seen','gs','alt_geom']]\t\n",
    "        df = df[df[\"type\"] == 'adsb_icao']\n",
    "        df['alt_baro'] = df['alt_baro'].apply(lambda x: 0 if (x == \"ground\" or x == np.nan ) else float(x))\n",
    "        df[\"nic\"] = df[\"nic\"].replace(np.NaN,0)\n",
    "\n",
    "        df = calculate_hexagon_ids(df)\n",
    "\n",
    "        mapping = {\n",
    "            \"flight\": str,\n",
    "            \"r\": str,\n",
    "            \"time\": \"datetime64[ns]\",\n",
    "            \"hex\": str,\n",
    "            \"Hexagon_ID\": str,\n",
    "            \"alt_baro\": float,\n",
    "            \"nic\": float,\n",
    "            \"good_bad\": bool,\n",
    "            \"lat\": float,\n",
    "            \"lon\": float,\n",
    "            \"type\": str,\n",
    "            \"t\": str,\n",
    "            \"category\": str,\n",
    "            \"version\": float,\n",
    "            \"nac_p\": float,\n",
    "            \"nac_v\": float,\n",
    "            \"track\": float,\n",
    "            \"baro_rate\": float,\n",
    "            \"seen_pos\": float,\n",
    "            \"seen\": float,\n",
    "            \"gs\": float,\n",
    "            \"alt_geom\": float\n",
    "        }\n",
    "        df=df.astype(mapping, copy=True)\n",
    "        df['good_bad'] = df['nic'].apply(lambda x: False if x==np.NaN or x<7 else True)\n",
    "        df = df[~df[\"flight\"].isnull()]\n",
    "\n",
    "        #save to pickle\n",
    "        if not os.path.exists(os.path.dirname(data[\"csv\"])):\n",
    "            os.makedirs(os.path.dirname(data[\"csv\"]))\n",
    "        df.to_csv(data[\"csv\"])\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle other exceptions\n",
    "        print(\"Get ADSB Data: An error occurred:\", e)\n",
    "\n",
    "def download_data(index, data, length_data_array):\n",
    "\n",
    "    try:\n",
    "        # if file exists, do nothing\n",
    "        if (os.path.isfile(data[\"csv\"])):\n",
    "            print(f\"csv exists {data['csv']}\")\n",
    "            return\n",
    "        else:\n",
    "            get_adsb_data(data)\n",
    "            return\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle other exceptions\n",
    "        print(\"Download data: An error occurred:\", e)\n",
    "\n",
    "def join_large_file(folder_path,start,end,file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "    isHeader = True\n",
    "    with open(file_path, 'w') as outfile:\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            file_datetime = datetime.strptime(file_path, 'csv\\\\%Y%m%d\\\\%H%M%SZ.csv')\n",
    "            print(file_datetime)\n",
    "            if filename.endswith('.csv') and start <= file_datetime and file_datetime <= end:\n",
    "                with open(file_path, 'r') as infile:\n",
    "                    # skip row header\n",
    "                    if isHeader:\n",
    "                        outfile.write(infile.read())\n",
    "                        isHeader = False\n",
    "                    else:\n",
    "                        outfile.write(''.join(infile.readlines()[1:]))\n",
    "                print(f'Joined {file_path}')\n",
    "        print(f\"Saved {file_path}\")\n",
    "\n",
    "def get_flight_data(start_date_time):\n",
    "    global df1\n",
    "    global flights_data\n",
    "    flights = df1[\"flight\"].unique()\n",
    "    flights = flights[1:]\n",
    "    display(f\"{flights.shape[0]} flights\")\n",
    "    flights_data = pd.DataFrame(flights, columns=['flight']).progress_apply(lambda x: process_flights_data(x['flight']), axis=1)\n",
    "    display(flights_data.head(10))\n",
    "    save_df(flights_data, start_date_time, filetype=FileType.FLIGHTS_DATA, parquet=True)\n",
    "    return flights_data\n",
    "\n",
    "def process_flights_data(flight):\n",
    "    global df1\n",
    "    rows = df1[df1['flight']==flight]\n",
    "    # print(f\"{flight} - {len(rows)} pings\")\n",
    "    bad_count = rows[rows['good_bad'] == False].shape[0]\n",
    "    total_count = rows.shape[0]\n",
    "    percentage_bad = bad_count/total_count if total_count!=0 else 0\n",
    "    result = analyse_data(percentage_bad)\n",
    "    return pd.Series({'flight': flight, 'bad_count': bad_count, 'total_count': total_count, 'percentage_bad': percentage_bad, 'result': result})\n",
    "\n",
    "    \n",
    "def process_data(start_date_time,end_date_time, reload=[False,False,False]):\n",
    "    delta = dt.timedelta(seconds=5)\n",
    "    start = start_date_time\n",
    "    global df1\n",
    "    global df2\n",
    "    global flights_data\n",
    "    file_path = os.path.join('csv', 'joined', f'{start_date_time.strftime(\"%Y%m%d\")}joined.csv')\n",
    "\n",
    "    if not reload[0]:\n",
    "\n",
    "        data_array = []\n",
    "        while start < end_date_time:\n",
    "            data_array.append({\n",
    "                \"url\": f'https://samples.adsbexchange.com/readsb-hist/{start.strftime(\"%Y/%m/%d\")}/{start.strftime(\"%H%M%S\")}Z.json.gz',\n",
    "                \"pickle\": os.path.join('csv',start.strftime(\"%Y%m%d\"), f'{start.strftime(\"%H%M%S\")}Z.pkl'),\n",
    "                \"csv\": os.path.join('csv',start.strftime(\"%Y%m%d\"), f'{start.strftime(\"%H%M%S\")}Z.csv')\n",
    "                })\n",
    "            start += delta\n",
    "\n",
    "        try:\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                l = [len(data_array)] * len(data_array)\n",
    "                list(executor.map(lambda i: download_data(i, data_array[i], l[i]), range(len(data_array))))\n",
    "        except Exception as e:\n",
    "            print(\"Thread Pool: An error occurred:\", e)\n",
    "            \n",
    "        join_large_file(os.path.join('csv',start_date_time.strftime(\"%Y%m%d\")),start_date_time,end_date_time,file_path)\n",
    "        csv_to_parquet(start_date_time, filetype = FileType.JOINED)\n",
    "\n",
    "    df1 = load_df(start_date_time, filetype=FileType.JOINED, parquet=True)\n",
    "    \n",
    "    flights_data = get_flight_data(start_date_time) if not reload[1] else load_df(start_date_time, filetype=FileType.FLIGHTS_DATA, parquet=True)\n",
    "\n",
    "    df2 = get_hexagon_df(start_date_time) if not reload[2] else load_df(start_date_time, filetype=FileType.HEXBIN, parquet=True)\n",
    "\n",
    "    return [df1, df2, flights_data]\n",
    "\n",
    "def get_hexagon_df(start_date_time):\n",
    "    global df1\n",
    "    global df2\n",
    "    df2 = df1.groupby(['Hexagon_ID', 'flight'], as_index=False)[['good_bad','alt_baro','time']].agg(\n",
    "                            total_count=('good_bad', lambda x: (x == x).sum()),\n",
    "                            bad_count=('good_bad', lambda x: (x == False).sum()),\n",
    "                            percentage_bad=('good_bad', lambda x: (x == False).sum()/(x==x).sum()),\n",
    "                            alt_baro_min=('alt_baro', lambda x: x.min()),\n",
    "                            alt_baro_max=('alt_baro', lambda x: x.max()),\n",
    "                            time_min=('time', lambda x: x.min()),\n",
    "                            time_max=('time', lambda x: x.max())\n",
    "                            )\n",
    "\n",
    "    def get_lat(row):\n",
    "        return h3.h3_to_geo(row['Hexagon_ID'])[0]\n",
    "\n",
    "    def get_lng(row):\n",
    "        return h3.h3_to_geo(row['Hexagon_ID'])[1]\n",
    "\n",
    "    # Apply the function to create new 'lat' and 'lng' columns\n",
    "    df2['lat'] = df2.apply(get_lat, axis=1)\n",
    "    df2['lng'] = df2.apply(get_lng, axis=1)\n",
    "\n",
    "    save_df(df2, start_date_time, filetype=FileType.HEXBIN, parquet=True)\n",
    "    return df2\n",
    "\n",
    "class FileType:\n",
    "    JOINED = \"joined\"\n",
    "    SAMPLED = \"sampled\"\n",
    "    HEXBIN = \"hexbin\"\n",
    "    FLIGHTS_DATA = \"flights_data\"\n",
    "\n",
    "def save_df(df, start_date_time, filetype=FileType.JOINED, parquet=False):\n",
    "    directory = 'joined'\n",
    "    if filetype == FileType.HEXBIN:\n",
    "        directory = 'hexbin'\n",
    "    if parquet:\n",
    "        file_path = os.path.join('csv', directory , f'{start_date_time.strftime(\"%Y%m%d\")}{filetype}.parquet')\n",
    "        print(f'Saving parquet {file_path}')\n",
    "        df.to_parquet(file_path) \n",
    "    else:\n",
    "        file_path = os.path.join('csv', directory , f'{start_date_time.strftime(\"%Y%m%d\")}{filetype}.csv')\n",
    "        print(f'Saving csv {file_path}')\n",
    "        if isinstance(df, np.ndarray):\n",
    "            df = pd.DataFrame(df)\n",
    "            df.to_csv(file_path) \n",
    "        else:\n",
    "            df.to_csv(file_path) \n",
    "\n",
    "def load_df(start_date_time, filetype=FileType.JOINED, parquet=False):\n",
    "    directory = 'joined'\n",
    "    if filetype == FileType.HEXBIN:\n",
    "        directory = 'hexbin'\n",
    "    if parquet:\n",
    "        file_path = os.path.join('csv', directory , f'{start_date_time.strftime(\"%Y%m%d\")}{filetype}.parquet')\n",
    "        print(f'Reading parquet {file_path}')\n",
    "        return pd.read_parquet(file_path)\n",
    "    else:\n",
    "        file_path = os.path.join('csv', directory , f'{start_date_time.strftime(\"%Y%m%d\")}{filetype}.csv')\n",
    "        return pd.concat([chunk for chunk in tqdm(pd.read_csv(file_path, chunksize=1000000), desc=f'Loading csv {file_path}')])\n",
    "\n",
    "def csv_to_parquet(start_date_time, filetype=FileType.JOINED):\n",
    "    chunk_size = 1000000\n",
    "    parquet_writer = pd.DataFrame()\n",
    "    directory = 'joined'\n",
    "    if filetype == FileType.HEXBIN:\n",
    "        directory = 'hexbin'\n",
    "    csv_path=os.path.join('csv', directory , f'{start_date_time.strftime(\"%Y%m%d\")}{filetype}.csv')\n",
    "    parquet_path=os.path.join('csv', directory , f'{start_date_time.strftime(\"%Y%m%d\")}{filetype}.parquet')\n",
    "    print(\"Converting csv to parquet\")\n",
    "    for i,chunk in enumerate(pd.read_csv(csv_path, chunksize=chunk_size)):\n",
    "        sys.stdout.write(\"Conversion progress: %d   \\r\" % (i) )\n",
    "        sys.stdout.flush()\n",
    "        parquet_writer = pd.concat([parquet_writer,chunk])\n",
    "    parquet_writer.to_parquet(parquet_path)\n",
    "\n",
    "def parquet_to_csv(start_date_time, filetype=FileType.JOINED):\n",
    "    print(\"Converting parquet to csv\")\n",
    "    df1=load_df(start_date_time, filetype=filetype, parquet=True)\n",
    "    save_df(df1, start_date_time, filetype=filetype, parquet=False)\n",
    "\n",
    "class ResultGroup:\n",
    "    EQUIPMENT_FAILURE = \"equipment_failure\"\n",
    "    NORMAL = \"normal\"\n",
    "    ANALYSE = \"analyse\"\n",
    "    UNKNOWN = \"unknown\"\n",
    "\n",
    "def generate_plots(sample_size, group = ResultGroup.ANALYSE):\n",
    "    global df1\n",
    "    global flights_data\n",
    "\n",
    "    if isinstance(df1, type(None)) or isinstance(flights_data, type(None)):\n",
    "        print(\"Generate plots: error no data\")\n",
    "        return\n",
    "\n",
    "    flights = [str(flight) for flight in flights_data[flights_data['result']==group].sample(sample_size)['flight']]\n",
    "    for flight in flights:\n",
    "        flight_df = df1[df1['flight']==flight]\n",
    "        flight_df.set_index(\"time\")\n",
    "\n",
    "        fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "        ax1.plot(flight_df['time'].astype(\"datetime64[ns]\"), flight_df['alt_baro'], 'bx-', label='alt_baro')\n",
    "        ax1.tick_params('y', colors='b')\n",
    "        ax1.set_yticks(np.arange(0, 50000, 5000))\n",
    "        ax1.xaxis.set_major_formatter(mdates.DateFormatter('%d-%m-%Y %H:%M:%S'))\n",
    "\n",
    "        ax1.set_xlabel('Date-Time', fontsize=10)\n",
    "        ax1.set_ylabel('Altitude (ft)', color='b', fontsize=10)\n",
    "\n",
    "        plt.gcf().autofmt_xdate()\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(flight_df['time'].astype(\"datetime64[ns]\"), flight_df['nic'], 'rx-', label='nic')\n",
    "        ax2.set_ylabel('NIC', color='g', fontsize=10)\n",
    "        ax2.tick_params('y', colors='g')\n",
    "        ax2.set_yticks(np.arange(0, 14, 1))\n",
    "\n",
    "        plt.title(f'Flight:{flight} Altitude and NIC vs Time  (categorised as {group})', fontsize=10)\n",
    "        fig.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "        url = f\"https://doc8643.com/aircraft/{flight_df.iloc[0]['t']}\"\n",
    "        print(url)\n",
    "\n",
    "def analyse_data(percentage_bad):\n",
    "    upper_limit=0.9\n",
    "    lower_limit=0.1\n",
    "    if percentage_bad>upper_limit:\n",
    "        return \"equipment_failure\"\n",
    "    elif percentage_bad<lower_limit:\n",
    "        return \"normal\"\n",
    "    elif percentage_bad>=lower_limit and percentage_bad<=upper_limit:\n",
    "        return \"analyse\"\n",
    "    else: \n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "# hexagon map\n",
    "def get_hexagon_grid(latitude, longitude, resolution, ring_size):\n",
    "    \"\"\"\n",
    "    Generate a hexagonal grid GeoDataFrame centered around a specified location.\n",
    "    Parameters:\n",
    "    - latitude (float): Latitude of the center point.\n",
    "    - longitude (float): Longitude of the center point.\n",
    "    - resolution (int): H3 resolution for hexagons.\n",
    "    - ring_size (int): Number of rings to create around the center hexagon.\n",
    "    Returns:\n",
    "    - hexagon_df (geopandas.GeoDataFrame): GeoDataFrame containing hexagons and their geometries.\n",
    "    \"\"\"\n",
    "\n",
    "    global hexagon_df\n",
    "\n",
    "    # Get the H3 hexagons covering the specified location\n",
    "    center_h3 = h3.geo_to_h3(latitude, longitude, resolution)\n",
    "    hexagons = list(h3.k_ring(center_h3, ring_size))  # Convert the set to a list\n",
    "\n",
    "    # Create a GeoDataFrame with hexagons and their corresponding geometries\n",
    "    hexagon_geometries = [shapely.geometry.Polygon(h3.h3_to_geo_boundary(hexagon, geo_json=True)) for hexagon in hexagons]\n",
    "    hexagon_df = gpd.GeoDataFrame({'Hexagon_ID': hexagons, 'geometry': hexagon_geometries})\n",
    "\n",
    "    \n",
    "def create_choropleth_map(alpha=0.3, map_style=\"carto-positron\", data=\"percentage_bad\", limits=[0,0.1,0.5,1], lat=home_lat, lon=home_lng):\n",
    "    \"\"\"\n",
    "    Create an interactive choropleth map using Plotly Express.\n",
    "    Parameters:\n",
    "    - geojson_df (GeoDataFrame): GeoJSON data containing polygon geometries.\n",
    "    - data_df (DataFrame): DataFrame containing data to be visualized on the map.\n",
    "    - alpha (float): Opacity level for the map polygons (0.0 to 1.0).\n",
    "    - map_style (str): Map style for the Plotly map (e.g., \"carto-positron\").\n",
    "    - color_scale (str): Color scale for the choropleth map.\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    global df2\n",
    "    global hexagon_df\n",
    "\n",
    "    # Hexagon grid around HOME\n",
    "    get_hexagon_grid(home_lat, home_lng, resolution, ring_size)\n",
    "    print(\"hexagons calculated\")\n",
    "\n",
    "    # Merge the GeoJSON data with your DataFrame\n",
    "    merged_df = hexagon_df.merge(df2, on=\"Hexagon_ID\", how=\"left\")\n",
    "\n",
    "    # Create a choropleth map using px.choropleth_mapbox\n",
    "    fig = px.choropleth_mapbox(\n",
    "        merged_df,\n",
    "        geojson=merged_df.geometry,\n",
    "        locations=merged_df.index,  # Use index as locations to avoid duplicate rows\n",
    "        color=data,\n",
    "        color_continuous_scale=[[limits[0], f'rgba(0,255,0,{alpha})'],\n",
    "                                [limits[1], f'rgba(255,255,0,{alpha})'],\n",
    "                                [limits[2], f'rgba(255,0,0,{alpha})'],\n",
    "                                [limits[3], f'rgba(255,0,0,{alpha})']],        \n",
    "        title=\"GPS Jam Map\",\n",
    "        mapbox_style=map_style,\n",
    "        center={\"lat\": home_lat, \"lon\": home_lng},  # Adjust the center as needed\n",
    "        zoom=2,\n",
    "    )\n",
    "\n",
    "    # Customize the opacity of the hexagons\n",
    "    fig.update_traces(marker=dict(opacity=alpha))\n",
    "\n",
    "    # Add hover data for hotel names\n",
    "    fig.update_traces(customdata=merged_df[[\"Hexagon_ID\",\"bad_count\", \"total_count\", \"percentage_bad\", \"lat\", \"lng\"]])\n",
    "\n",
    "    # Define the hover template \n",
    "    hover_template = \"<b>Hexagon ID:</b> %{customdata[0]}<br><b>Location:</b> %{customdata[4]:.4f},%{customdata[5]:.4f}<br><b>Percentage bad:</b> %{customdata[3]:.3f}<br><b>Total Count:</b> %{customdata[2]}<extra></extra>\"\n",
    "    fig.update_traces(hovertemplate=hover_template)\n",
    "\n",
    "    # Set margins to 25 on all sides\n",
    "    fig.update_layout(margin=dict(l=35, r=35, t=45, b=35))\n",
    "    \n",
    "    # Adjust the width of the visualization\n",
    "    fig.update_layout(width=1000) \n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# note that for the free sample, only the first day of each month is available on the adsbexchange\n",
    "# 4 hours + to download 1 day of files\n",
    "# 18 mins to reload\n",
    "\n",
    "# Reload flags are for skipping steps and saving processing time\n",
    "# reload[0]=True loads data from [date]joined.csv (df1)\n",
    "# reload[1]=True loads data from [date]flights_data.csv (>45 min) (flights_data)\n",
    "# reload[2]=True loads data from [date]hexbin.csv (df2)\n",
    "\n",
    "# [df1, df2, flights_data] = process_data(dt.datetime(2024,5,1,0,0,0),dt.datetime(2024,5,2,0,0,0), reload=[False, False, False])\n",
    "start_date_time = dt.datetime(2024,6,1,0,0,0)\n",
    "\n",
    "csv_to_parquet(start_date_time, filetype=FileType.JOINED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Plots For Sampled Flights, Alt/NIC vs Time\n",
    "start_date_time = dt.datetime(2024,4,1,0,0,0)\n",
    "df1 = load_df(start_date_time, filetype=FileType.JOINED, parquet=False)\n",
    "flights_data = load_df(start_date_time, filetype=FileType.FLIGHTS_DATA, parquet=False)\n",
    "display(flights_data['result'].value_counts())\n",
    "\n",
    "# > 0.9: Equipment Failure\n",
    "# Between 0.1 and 0.9: Analyse\n",
    "# < 0.1: Normal\n",
    "# generate_plots(5, group=ResultGroup.NORMAL)\n",
    "generate_plots(5, group=ResultGroup.ANALYSE)\n",
    "# generate_plots(5, group=ResultGroup.EQUIPMENT_FAILURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_time = dt.datetime(2024,4,1,0,0,0)\n",
    "df1 = load_df(start_date_time, filetype=FileType.JOINED, parquet=False)\n",
    "flights_data = load_df(start_date_time, filetype=FileType.FLIGHTS_DATA, parquet=False)\n",
    "\n",
    "def generate_correlations(sample_size, group = ResultGroup.ANALYSE):\n",
    "    global df1\n",
    "    global flights_data\n",
    "\n",
    "    if isinstance(df1, type(None)) or isinstance(flights_data, type(None)):\n",
    "        print(\"Generate plots: error no data\")\n",
    "        return\n",
    "\n",
    "    flights = [str(flight) for flight in flights_data[flights_data['result']==group].sample(sample_size)['flight']]\n",
    "    flights_df=None\n",
    "    for flight in flights:\n",
    "        flight_df = df1[df1['flight']==flight]\n",
    "        flights_df = pd.concat([flights_df,flight_df])    \n",
    "    \n",
    "    flights_df.set_index(\"time\")\n",
    "    print(flights_df.shape[0])\n",
    "    display(flights_df.head(10))\n",
    "\n",
    "    corr_matrix = flights_df.corr(method='pearson')\n",
    "    ax = plt.axes()\n",
    "    ax.set_title(f'Pearson cofficient for sample size {sample_size}, \\ngroup: {group} \\nflights: {flights} ')\n",
    "    sn.heatmap(corr_matrix, annot=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots Hex Map\n",
    "\n",
    "start_date_time = dt.datetime(2024,4,1,0,0,0)\n",
    "df1 = load_df(start_date_time, filetype=FileType.JOINED, parquet=False)\n",
    "df2 = get_hexagon_df(start_date_time)\n",
    "display(df2.head(10))\n",
    "\n",
    "coord = [33.81825, 35.49082]\n",
    "hexagon_ID = h3.geo_to_h3(coord[0], coord[1], resolution=resolution)\n",
    "print(f\"{coord}->{hexagon_ID}\")\n",
    "create_choropleth_map(data=\"percentage_bad\", limits = [0,0.1,0.5,1], lat=coord[0], lon=coord[1])\n",
    "# create_choropleth_map(data=\"bad_count\", limits = [0,20,50,100])\n",
    "\n",
    "flights_in_hex = df1[df1['hexagon_ID']==hexagon_ID]['flight'].unique()\n",
    "\n",
    "flights_df=None\n",
    "for flight in flights_in_hex:\n",
    "        flight_df = df1[df1['flight']==flight and df1['hexagon_ID']==hexagon_ID]\n",
    "        time_start = flight_df[\"time\"].min()\n",
    "        time_end = flight_df[\"time\"].max()\n",
    "        # flight_df2 contains all pings of the flight between the start and end time that the flight is inside the hex. i.e. it includes pings that have null lat/lon values\n",
    "        flight_df2 = df1[df1['flight']==flight and df1['time'].between(time_start, time_end)]\n",
    "        flights_df = pd.concat([flights_df,flight_df2])\n",
    "\n",
    "display(flights_df.head(100))\n",
    "\n",
    "# given lat/lon -> find hexagon_ID -> filter all flights\n",
    "# plot flights on hexagon map\n",
    "\n",
    "# plot Altitude / NIC vs time\n",
    "\n",
    "# 2024-04-01\n",
    "# 33.81825, 35.49082\n",
    "# Label 318/Size 63600\n",
    "# 30.0842, 31.35159\n",
    "# Label 281/Size 56200\n",
    "\n",
    "\n",
    "# 2024-02-01\n",
    "# 33.81825, 35.49082\n",
    "# Label 200/Size 40000\n",
    "\n",
    "# 2024-03-01\n",
    "# 33.81825, 35.49082\n",
    "# Label 120/Size 24000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO\n",
    "# 1. .info() to get data types  or .dtypes  \n",
    "# 2. .describe() 0> gets mean() count(), 25%, 50%, 75% max min\n",
    "# 3. .isnull() check for missing values   ->  isnull().sum().sort_values(ascending=False)\n",
    "# 4. .nunique() -> gets unique values for each field with their counts\n",
    "# 5. .unique() -> gets a list of unique values in a column\n",
    "# 6. .value_counts() -> gets unique values for the column with their counts\n",
    "# 7. for name, group in grouped: -> iterate over groups\n",
    "# 8. pandas time series functions: Timestamp->DatetimeIndex[] (to_datetime, date_range), Timedelta->TimedeltaIndex[] (to_timedelta, timedelta_range), Period->PeriodIndex[] (Period, period_range), DateOffset (Dateoffset)\n",
    "# 9. Missing Values: Numeric (pd.nan), String (pd.NA), Time (pd.NaT) -> use .isna(), .notna() to detect missing values\n",
    "\n",
    "# DON'T DO\n",
    "# 1. seems like saving as a pickle may corrupt the data, I see a lot of NA, maybe due to the data conversion between types. avoid any data conversion except at the start\n",
    "# 2. use display() instead of print() \n",
    "# 3. .dropna() -> WRONG USE, drops any row or column with a missing value!\n",
    "# 4. df[df[]] is a filter, not selecting the rows. it is wrong\n",
    "\n",
    "# flights time Hexagon_ID good_bad lat lon\n",
    "# problem: some lat lon are NaN because of GPS failure. To assign nearest hexagon_ID based on nearest time with lat lon\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight = str(flights_data[flights_data['result']=='analyse'].sample(1).iloc[0]['flight'])\n",
    "print(flight)\n",
    "\n",
    "flight_df = df1[df1['flight']==flight]\n",
    "print(flight_df.shape[0])\n",
    "display(flight_df.head(10))\n",
    "flight_df.set_index(\"time\")\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "ax1.plot(flight_df['time'], flight_df['alt_baro'], 'b-', label='alt_baro')\n",
    "ax1.tick_params('y', colors='b')\n",
    "ax1.set_yticks(np.arange(0, 50000, 5000))\n",
    "\n",
    "ax1.set_xlabel('Date-Time', fontsize=12)\n",
    "ax1.set_ylabel('Altitude (ft)', color='b', fontsize=12)\n",
    "\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(flight_df['time'], flight_df['nic'], 'rx', label='nic')\n",
    "ax2.set_ylabel('NIC', color='g', fontsize=12)\n",
    "ax2.tick_params('y', colors='g')\n",
    "ax2.set_yticks(np.arange(0, 14, 1))\n",
    "\n",
    "plt.title('Altitude and NIC vs Time')\n",
    "fig.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_choropleth_map(data_df=df2, data=\"percentage_bad\", limits = [0,0.1,0.5,1], )\n",
    "# create_choropleth_map(data_df=df2, data=\"bad_count\", limits = [0,20,50,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://towardsdatascience.com/3-python-packages-for-interactive-data-analysis-3063a201a589\n",
    "# https://towardsdatascience.com/4-libraries-that-can-perform-eda-in-one-line-of-python-code-b13938a06ae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Change in NIC/NAC/SIL indicates an anomaly, which may be due to any reason\n",
    "\n",
    "# nic: Navigation Integrity Category (2.2.3.2.7.2.6)\n",
    "# Table 1: NIC value and corresponding size of containment radius\n",
    "# NIC Containment Radius\n",
    "# 0 Unknown\n",
    "# 1 Rc < 37.04km (20nm)\n",
    "# 2 Rc < 14.816km (8nm)\n",
    "# 3 Rc < 7.408km (4nm)\n",
    "# 4 Rc < 3.704km (2nm)\n",
    "# 5 Rc < 1852m (1nm)\n",
    "# 6 Rc < 1111.2m (0.6nm)\n",
    "# Rc < 926m (0.5nm)\n",
    "# Rc < 555.6m (0.3nm)\n",
    "# 7 Rc < 370.4m (0.2nm)\n",
    "# 8 Rc < 185.2m (0.1nm)\n",
    "# 9 Rc < 75m\n",
    "# 10 Rc < 25m\n",
    "# 11 Rc < 7.5m\n",
    "\n",
    "# Assuming you have already calculated the counts\n",
    "# counts = df1['nic'].value_counts(dropna=False)\n",
    "# print(f\"List all nic values with counts {counts}\")\n",
    "# # Create bins for the histogram\n",
    "# bins = np.arange(len(counts) + 1)\n",
    "\n",
    "# # Plot the histogram with NA on the left\n",
    "# plt.hist(df1['nic'], bins=bins, align='left',rwidth=0.5)\n",
    "# plt.yscale('log')\n",
    "# plt.title('nic')\n",
    "# plt.show()\n",
    "\n",
    "print(f\"bad {df1[df1['good_bad']=='bad'].shape[0]}\")\n",
    "print(f\"total {df1.shape[0]}\")\n",
    "print(f\"% bad / total {df1[df1['good_bad']=='bad'].shape[0]/df1.shape[0]*100:.2f}%\")\n",
    "df1[df1['good_bad']=='bad'].head(100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "print(f\"List all type with counts {df1['type'].value_counts()}\")\n",
    "# List all type with counts type\n",
    "# adsb_icao         9145 - messages from a Mode S or ADS-B transponder, using a 24-bit ICAO address\n",
    "# other              725 - IGNORE miscellaneous data received via Basestation / SBS format, quality / source is unknown.\n",
    "# adsb_icao_nt       524 - IGNORE - messages from an ADS-B equipped “non-transponder” emitter e.g. a ground vehicle, using a 24-bit ICAO address\n",
    "# mode_s             515 - ModeS data from the planes transponder (no position transmitted)\n",
    "# adsr_icao          280 - rebroadcast of ADS-B messages originally sent via another data link e.g. UAT, using a 24-bit ICAO address\n",
    "# tisb_other         256 - traffic information about a non-ADS-B target using a non-ICAO address\n",
    "# tisb_trackfile     214 - traffic information about a non-ADS-B target using a track/file identifier, typically from primary or Mode A/C radar\n",
    "# mlat               116 - MLAT, position calculated arrival time differences using multiple receivers, outliers and varying accuracy is expected.\n",
    "# unknown             49\n",
    "# tisb_icao           31 - traffic information about a non-ADS-B target identified by a 24-bit ICAO address, e.g. a Mode S target tracked by secondary radar\n",
    "# adsb_other          17 - messages from an ADS-B transponder using a non-ICAO address, e.g. anonymized address\n",
    "plt.hist(df1['type'], align='left')\n",
    "plt.title('type')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# category: emitter category to identify particular aircraft or vehicle classes (values A0 – D7) (2.2.3.2.5.2)\n",
    "counts = df1['category'].dropna()\n",
    "# print(f\"List all category with counts {counts}\")\n",
    "plt.hist(counts)\n",
    "plt.title('category')\n",
    "plt.show()\n",
    "# A0 : No ADS-B emitter category information. Do not use this emitter category. If no emitter category fits your installation, seek guidance from the FAA as appropriate. A1 : Light (< 15500 lbs) – Any airplane with a maximum takeoff weight less than 15,500 pounds. This includes very light aircraft (light sport aircraft) that do not meet the requirements of 14 CFR § 103.1.\n",
    "# A2 : Small (15500 to 75000 lbs) – Any airplane with a maximum takeoff weight greater than or equal to15,500 pounds but less than 75,000 pounds.\n",
    "# A3 : Large (75000 to 300000 lbs) – Any airplane with a maximum takeoff weight greater than or equal to 75,000 pounds but less than 300,000 pounds that does not qualify for the high vortex category.\n",
    "# A4 :  High vortex large (aircraft such as B-757) – Any airplane with a maximum takeoff weight greater than or equal to 75,000 pounds but less than 300,000 pounds that has been determined to generate a high wake vortex. Currently, the Boeing 757 is the only example.\n",
    "# A5 : Heavy (> 300000 lbs) – Any airplane with a maximum takeoff weight equal to or above 300,000 pounds.\n",
    "# A6 : High performance (> 5g acceleration and 400 kts) – Any airplane, regardless of weight, which can maneuver in excess of 5 G’s and maintain true airspeed above 400 knots.\n",
    "# A7 : Rotorcraft – Any rotorcraft regardless of weight.\n",
    "# B0 : No ADS-B emitter category information\n",
    "# B1 : Glider / sailplane – Any glider or sailplane regardless of weight.\n",
    "# B2 : Lighter-than-air – Any lighter than air (airship or balloon) regardless of weight.\n",
    "# B3 : Parachutist / skydiver\n",
    "# B4 : Ultralight / hang-glider / paraglider – A vehicle that meets the requirements of 14 CFR § 103.1. Light sport aircraft should not use the ultralight emitter category unless they meet 14 CFR § 103.1.\n",
    "# B5 : Reserved\n",
    "# B6 : Unmanned aerial vehicle – Any unmanned aerial vehicle or unmanned aircraft system regardless of weight.\n",
    "# B7 : Space / trans-atmospheric vehicle\n",
    "# C0 : No ADS-B emitter category information\n",
    "# C1 : Surface vehicle – emergency vehicle\n",
    "# C2 : Surface vehicle – service vehicle\n",
    "# C3 : Point obstacle (includes tethered balloons)\n",
    "# C4 : Cluster obstacle\n",
    "# C5 : Line obstacle\n",
    "# C6 : Reserved\n",
    "# C7 : Reserved\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
