{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m venv venv\n",
    "# # !venv/scripts/activate\n",
    "%pip install pandas numpy matplotlib seaborn plotly geopandas scipy scikit-learn statsmodels requests aiohttp shapely ipython h3 ipywidgets dtale tqdm pickle\n",
    "\n",
    "%pip install --upgrade nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "from pandas import read_csv\n",
    "import concurrent.futures\n",
    "import psutil\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import h3\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "import plotly.express as px\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from pandas import read_pickle\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import seaborn as sn\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Latitude and longitude coordinates\n",
    "home_lat = 1.3472764\n",
    "home_lng = 103.9104234\n",
    "\n",
    "# Generate H3 hexagons at a specified resolution (e.g., 9)\n",
    "resolution = 5\n",
    "\n",
    "# Indicate the number of rings around the central hexagon\n",
    "ring_size = 463\n",
    "\n",
    "# global dataframes to avoid keep loading files\n",
    "df1=None\n",
    "df2=None\n",
    "flights_data=None\n",
    "hexagon_df=None\n",
    "flights_df=None\n",
    "hex_list_df = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the hexagon grid is suitable\n",
    "# create_choropleth_map_only_hexagon_df(geojson_df=hexagon_df)\n",
    "\n",
    "# def create_choropleth_map_only_hexagon_df(geojson_df, alpha=0.1, map_style=\"carto-positron\", color_scale=\"Viridis\"):\n",
    "#     \"\"\"\n",
    "#     Create an interactive choropleth map using Plotly Express.\n",
    "#     Parameters:\n",
    "#     - geojson_df (GeoDataFrame): GeoJSON data containing polygon geometries.\n",
    "#     - alpha (float): Opacity level for the map polygons (0.0 to 1.0).\n",
    "#     - map_style (str): Map style for the Plotly map (e.g., \"carto-positron\").\n",
    "#     - color_scale (str): Color scale for the choropleth map.\n",
    "#     Returns:\n",
    "#     None\n",
    "#     \"\"\"\n",
    "#     # Create a choropleth map using px.choropleth_mapbox\n",
    "#     fig = px.choropleth_mapbox(\n",
    "#         geojson_df,\n",
    "#         geojson=geojson_df.geometry,\n",
    "#         locations=geojson_df.index,  # Use index as locations to avoid duplicate rows\n",
    "#         # color=\"Count\",\n",
    "#         color_continuous_scale=color_scale,\n",
    "#         title=\"GPS Jam Map\",\n",
    "#         mapbox_style=map_style,\n",
    "#         center={\"lat\": home_lat, \"lon\": home_lng},  # Adjust the center as needed\n",
    "#         zoom=2,\n",
    "#     )\n",
    "\n",
    "#     # Customize the opacity of the hexagons\n",
    "#     fig.update_traces(marker=dict(opacity=alpha))\n",
    "\n",
    "#     # Set margins to 25 on all sides\n",
    "#     fig.update_layout(margin=dict(l=35, r=35, t=45, b=35))\n",
    "    \n",
    "#     # Adjust the width of the visualization\n",
    "#     fig.update_layout(width=1000) \n",
    "\n",
    "#     fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/170945Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171040Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171120Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171045Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171035Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171020Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171115Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171030Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/170955Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171000Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171015Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171110Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171105Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171025Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171050Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171010Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171100Z.json.gzdownloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/170950Z.json.gz\n",
      "\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171005Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171055Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171250Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171150Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171135Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171240Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171255Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171215Z.json.gzdownloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171205Z.json.gz\n",
      "\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171235Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171125Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171225Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171145Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171220Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171300Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171155Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171230Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171140Z.json.gzdownloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171245Z.json.gz\n",
      "\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171130Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171210Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171200Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171310Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171410Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171435Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171425Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171330Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171430Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171315Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171320Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171345Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171400Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171325Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171415Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171305Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171440Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171340Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171335Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171420Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171405Z.json.gz\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171355Z.json.gzdownloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171350Z.json.gz\n",
      "\n",
      "downloaded https://samples.adsbexchange.com/readsb-hist/2024/04/01/171520Z.json.gz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_hexagon_ids(df):\n",
    "    \"\"\"\n",
    "    Calculate Hexagon IDs for each row(ping) in a DataFrame based on their geographic coordinates.\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing ADSB data with \"lat\" and \"lon\" columns.\n",
    "        hexagon_df (gpd.GeoDataFrame): GeoDataFrame with hexagon geometries and associated Hexagon IDs.\n",
    "    Returns:\n",
    "        pd.DataFrame: The input DataFrame with an additional \"Hexagon_ID\" column indicating the Hexagon ID for each ping.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a column Hexagon_ID with the ID of the hexagon\n",
    "    df['Hexagon_ID'] = None\n",
    "\n",
    "    df = df.sort_values(['flight','time'])\n",
    "\n",
    "    # Iterate through the rows in the df DataFrame and calculate counts within each hexagon\n",
    "    for i, ping in df.iterrows():\n",
    "        if not isinstance(ping['lat'], float):\n",
    "            # use location of last ping\n",
    "            found = False\n",
    "            j=1\n",
    "            while not found:\n",
    "                if isinstance(df.loc[i-j,'lat'],float):\n",
    "                    df.loc[i,'lat']=df.loc[i-j,'lat']\n",
    "                    df.loc[i,'lon']=df.loc[i-j,'lon']\n",
    "                    found=True\n",
    "                elif isinstance(df.loc[i+j,'lat'],float):\n",
    "                    df.loc[i,'lat']=df.loc[i+j,'lat']\n",
    "                    df.loc[i,'lon']=df.loc[i+j,'lon']\n",
    "                    found=True\n",
    "                else:\n",
    "                    j+=1\n",
    "            continue\n",
    "        resolution=5   \n",
    "        result = h3.geo_to_h3(ping[\"lat\"], ping[\"lon\"], resolution)\n",
    "        # print(f'{ping[\"lat\"]},{ping[\"lon\"]}=>{result}')\n",
    "        if result != 0:\n",
    "             df.loc[i, 'Hexagon_ID'] = result\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_adsb_data(data):\n",
    "    \"\"\"\n",
    "    Fetches data from ADSB web api. Filter adsb_icao. Clean up non-numeric values. \n",
    "    Assign Hexagon_IDs to each ping. Convert types per the mapping. Filter where flight is null. \n",
    "    Assign \"good_bad\" is True when NIC >= 7, False when NIC <= 6\n",
    "    Saves data to csv.\n",
    "    Args:\n",
    "        data: Dictionary of url and csv paths\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if (os.path.isfile(data[\"csv\"])):\n",
    "        print(f\"csv exists {data['csv']}\")\n",
    "        return\n",
    "    else:\n",
    "        try:\n",
    "            response = requests.get(data[\"url\"])\n",
    "            print(f\"downloaded {data['url']}\")\n",
    "            json_data = response.json()\n",
    "            \n",
    "            df = pd.json_normalize(json_data['aircraft'])\n",
    "\n",
    "            df = df[df[\"type\"] == 'adsb_icao']\n",
    "            df['time'] = pd.to_datetime(datetime.fromtimestamp(json_data['now']).strftime(\"%Y%m%d%H%M%S\"))\n",
    "            df['Hexagon_ID'] = pd.NA\n",
    "            df['good_bad'] = np.NaN\n",
    "            df = df[['flight','r','time', 'hex', 'Hexagon_ID', 'alt_baro', 'nic', 'good_bad','lat','lon','type','t','category','version','nac_p','nac_v','track','baro_rate','seen_pos','seen','gs','alt_geom']]\t\n",
    "            df = df[df[\"type\"] == 'adsb_icao']\n",
    "            df['alt_baro'] = df['alt_baro'].apply(lambda x: 0 if (x == \"ground\" or x == np.nan ) else float(x))\n",
    "            df[\"nic\"] = df[\"nic\"].replace(np.NaN,0)\n",
    "\n",
    "            mapping = {\n",
    "                \"flight\": str,\n",
    "                \"r\": str,\n",
    "                \"time\": \"datetime64[ns]\",\n",
    "                \"hex\": str,\n",
    "                \"Hexagon_ID\": str,\n",
    "                \"alt_baro\": float,\n",
    "                \"nic\": float,\n",
    "                \"good_bad\": bool,\n",
    "                \"lat\": float,\n",
    "                \"lon\": float,\n",
    "                \"type\": str,\n",
    "                \"t\": str,\n",
    "                \"category\": str,\n",
    "                \"version\": float,\n",
    "                \"nac_p\": float,\n",
    "                \"nac_v\": float,\n",
    "                \"track\": float,\n",
    "                \"baro_rate\": float,\n",
    "                \"seen_pos\": float,\n",
    "                \"seen\": float,\n",
    "                \"gs\": float,\n",
    "                \"alt_geom\": float\n",
    "            }\n",
    "            df=df.astype(mapping, copy=True)\n",
    "            df['good_bad'] = df['nic'].apply(lambda x: False if x==np.NaN or x<7 else True)\n",
    "            df = df[~df[\"flight\"].isnull()]\n",
    "\n",
    "            df = calculate_hexagon_ids(df)\n",
    "\n",
    "            if not os.path.exists(os.path.dirname(data[\"csv\"])):\n",
    "                os.makedirs(os.path.dirname(data[\"csv\"]))\n",
    "            df.to_csv(data[\"csv\"])\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle other exceptions\n",
    "            print(\"Get ADSB Data: An error occurred:\", e)\n",
    "\n",
    "\n",
    "def join_large_file(folder_path,start,end,file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "    isHeader = True\n",
    "    with open(file_path, 'w') as outfile:\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            file_datetime = datetime.strptime(file_path, 'csv\\\\%Y%m%d\\\\%H%M%SZ.csv')\n",
    "            print(file_datetime)\n",
    "            if filename.endswith('.csv') and start <= file_datetime and file_datetime <= end:\n",
    "                with open(file_path, 'r') as infile:\n",
    "                    # skip row header\n",
    "                    if isHeader:\n",
    "                        outfile.write(infile.read())\n",
    "                        isHeader = False\n",
    "                    else:\n",
    "                        outfile.write(''.join(infile.readlines()[1:]))\n",
    "                print(f'Joined {file_path}')\n",
    "        print(f\"Saved {file_path}\")\n",
    "\n",
    "def get_flight_data(start_date_time):\n",
    "    \"\"\"\n",
    "    Creates a table of flights and calculates stats for each flight based on aggregates over the rows\n",
    "    Args:\n",
    "        start_date_time: start time e.g. dt.datetime(YYYY,MM,DD,HH,MM,SS)\n",
    "    Global:\n",
    "        df1: dataframe of raw flight pings from ADSB\n",
    "        global flights_data\n",
    "    Returns:\n",
    "        flights_data: one row for each flight with aggregated fields 'flight', 'bad_count', 'total_count', 'percentage_bad','result'\n",
    "    \"\"\"\n",
    "    global df1\n",
    "    global flights_data\n",
    "    flights = df1[\"flight\"].unique()\n",
    "    flights = flights[1:]\n",
    "    display(f\"Found {flights.shape[0]} flights. Preparing data by flights\")\n",
    "    flights_data = pd.DataFrame(flights, columns=['flight']).progress_apply(lambda x: process_flights_data(x['flight']), axis=1)\n",
    "    display(flights_data.head(10))\n",
    "    save_df(flights_data, start_date_time, filetype=FileType.FLIGHTS_DATA, parquet=True)\n",
    "    return flights_data\n",
    "\n",
    "def process_flights_data(flight):\n",
    "    \"\"\"\n",
    "    Extract \"rows\" for this flight from the large df1 table. Calculate counts so that the flight cas be classified as \"normal\", \"equipment failure\", \"analyse\" or \"unknown\"\n",
    "    This function is called inside a pandas.apply on flights_data.\n",
    "    Args:\n",
    "        flight: string of Flight Identifier\n",
    "    Globals:\n",
    "        df1: dataframe of raw flight pings from ADSB\n",
    "    Returns:\n",
    "        pd.Series: one row of flights_data dataframe with aggregated fields 'flight', 'bad_count', 'total_count', 'percentage_bad','result'\n",
    "    \"\"\"\n",
    "    global df1\n",
    "    rows = df1[df1['flight']==flight]\n",
    "    # print(f\"{flight} - {len(rows)} pings\")\n",
    "    bad_count = rows[rows['good_bad'] == False].shape[0]\n",
    "    total_count = rows.shape[0]\n",
    "    percentage_bad = bad_count/total_count if total_count!=0 else 0\n",
    "    result = analyse_data(percentage_bad)\n",
    "    return pd.Series({'flight': flight, 'bad_count': bad_count, 'total_count': total_count, 'percentage_bad': percentage_bad, 'result': result})\n",
    "\n",
    "    \n",
    "def process_data(start_date_time,end_date_time, reload=[False,False,False]):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        start_date_time: start time e.g. dt.datetime(YYYY,MM,DD,HH,MM,SS)\n",
    "        end_date_time: end time e.g. dt.datetime(YYYY,MM,DD,HH,MM,SS)\n",
    "        reload=[False,False,False]:\n",
    "            Reload flags are for skipping steps and saving processing time\n",
    "            reload[0]=True loads data from [date]joined.csv (df1)\n",
    "            reload[1]=True loads data from [date]flights_data.csv (>45 min) (flights_data)\n",
    "            reload[2]=True loads data from [date]hexbin.csv (df2)\n",
    "    Returns:\n",
    "        [df1, df2, flights_data]\n",
    "        df1: dataframe of 'flight','r','time', 'hex', 'Hexagon_ID', 'alt_baro', 'nic', 'good_bad',\n",
    "            'lat','lon','type','t','category','version','nac_p','nac_v','track','baro_rate','seen_pos',\n",
    "            'seen','gs','alt_geom'\n",
    "        flights_data: dataframe of  'flight','bad_count', 'total_count', 'percentage_bad', 'result'\n",
    "        df2: dataframe of 'Hexagon_ID', 'total_count, 'bad_count', 'alt_baro_range', 'time_range', \n",
    "            'percentage_bad'\n",
    "\n",
    "    \"\"\"\n",
    "    delta = dt.timedelta(seconds=5)\n",
    "    start = start_date_time\n",
    "    global df1\n",
    "    global df2\n",
    "    global flights_data\n",
    "    file_path = os.path.join('csv', 'joined', f'{start_date_time.strftime(\"%Y%m%d\")}joined.csv')\n",
    "\n",
    "    if not reload[0]:\n",
    "\n",
    "        data_array = []\n",
    "        while start < end_date_time:\n",
    "            data_array.append({\n",
    "                \"url\": f'https://samples.adsbexchange.com/readsb-hist/{start.strftime(\"%Y/%m/%d\")}/{start.strftime(\"%H%M%S\")}Z.json.gz',\n",
    "                \"csv\": os.path.join('csv',start.strftime(\"%Y%m%d\"), f'{start.strftime(\"%H%M%S\")}Z.csv')\n",
    "                })\n",
    "            start += delta\n",
    "\n",
    "        try:\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                list(executor.map(lambda i: get_adsb_data(data_array[i]), range(len(data_array))))\n",
    "        except Exception as e:\n",
    "            print(\"Thread Pool: An error occurred:\", e)\n",
    "            \n",
    "        join_large_file(os.path.join('csv',start_date_time.strftime(\"%Y%m%d\")),start_date_time,end_date_time,file_path)\n",
    "        csv_to_parquet(start_date_time, filetype = FileType.JOINED)\n",
    "\n",
    "    df1 = load_df(start_date_time, filetype=FileType.JOINED, parquet=True)\n",
    "    \n",
    "    flights_data = get_flight_data(start_date_time) if not reload[1] else load_df(start_date_time, filetype=FileType.FLIGHTS_DATA, parquet=True)\n",
    "\n",
    "    df2 = get_hexagon_df(start_date_time) if not reload[2] else load_df(start_date_time, filetype=FileType.HEXBIN, parquet=True)\n",
    "\n",
    "    return [df1, flights_data, df2]\n",
    "\n",
    "\n",
    "def get_hexagon_df(start_date_time):\n",
    "    global df1\n",
    "    global df2\n",
    "    df2 = df1.groupby(['Hexagon_ID', 'flight'], as_index=False)[['good_bad','alt_baro','time']].agg(\n",
    "                            total_count=('good_bad', lambda x: (x == x).sum()),\n",
    "                            bad_count=('good_bad', lambda x: (x == False).sum()),\n",
    "                            percentage_bad=('good_bad', lambda x: (x == False).sum()/(x==x).sum()),\n",
    "                            alt_baro_min=('alt_baro', lambda x: x.min()),\n",
    "                            alt_baro_max=('alt_baro', lambda x: x.max()),\n",
    "                            time_min=('time', lambda x: x.min()),\n",
    "                            time_max=('time', lambda x: x.max())\n",
    "                            )\n",
    "    df2 = pd.DataFrame(df2)\n",
    "    df2[\"Hexagon_ID\"]=df2[\"Hexagon_ID\"].replace(\"0\",\"NA\")\n",
    "    display(df2.head(10))\n",
    "\n",
    "    # Apply the function to create new 'lat' and 'lng' columns\n",
    "    coord = df2['Hexagon_ID'].apply(lambda x: h3.h3_to_geo(x) if x!=\"NA\" else (np.nan, np.nan))\n",
    "    df2[['lat', 'lon']] = pd.DataFrame(coord.tolist(), index=df2.index)\n",
    "\n",
    "    # coord = df2['Hexagon_ID'].apply(lambda x: h3.h3_to_geo(x) if x!=\"NA\" else pd.Series([np.nan, np.nan]))\n",
    "    # df2['lat']=coord[0]\n",
    "    # df2['lon']=coord[1]\n",
    "    \n",
    "    save_df(df2, start_date_time, filetype=FileType.HEXBIN, parquet=True)\n",
    "    return df2\n",
    "\n",
    "\n",
    "class FileType:\n",
    "    JOINED = \"joined\"\n",
    "    SAMPLED = \"sampled\"\n",
    "    HEXBIN = \"hexbin\"\n",
    "    FLIGHTS_DATA = \"flights_data\"\n",
    "\n",
    "def save_df(df, start_date_time, filetype=FileType.JOINED, parquet=False):\n",
    "    directory = 'joined'\n",
    "    if parquet:\n",
    "        file_path = os.path.join('csv', directory , f'{start_date_time.strftime(\"%Y%m%d\")}{filetype}.parquet')\n",
    "        display(f'Saving parquet {file_path}')\n",
    "        df.to_parquet(file_path) \n",
    "    else:\n",
    "        file_path = os.path.join('csv', directory , f'{start_date_time.strftime(\"%Y%m%d\")}{filetype}.csv')\n",
    "        display(f'Saving csv {file_path}')\n",
    "        if isinstance(df, np.ndarray):\n",
    "            df = pd.DataFrame(df)\n",
    "            df.to_csv(file_path) \n",
    "        else:\n",
    "            df.to_csv(file_path) \n",
    "\n",
    "def load_df(start_date_time, filetype=FileType.JOINED, parquet=False):\n",
    "    directory = 'joined'\n",
    "    if parquet:\n",
    "        file_path = os.path.join('csv', directory , f'{start_date_time.strftime(\"%Y%m%d\")}{filetype}.parquet')\n",
    "        display(f'Reading parquet {file_path}')\n",
    "        return pd.read_parquet(file_path)\n",
    "    else:\n",
    "        file_path = os.path.join('csv', directory , f'{start_date_time.strftime(\"%Y%m%d\")}{filetype}.csv')\n",
    "        return pd.concat([chunk for chunk in tqdm(pd.read_csv(file_path, chunksize=1000000), desc=f'Loading csv {file_path}')])\n",
    "\n",
    "def csv_to_parquet(start_date_time, filetype=FileType.JOINED):\n",
    "    chunk_size = 1000000\n",
    "    parquet_writer = pd.DataFrame()\n",
    "    directory = 'joined'\n",
    "    if filetype == FileType.HEXBIN:\n",
    "        directory = 'hexbin'\n",
    "    csv_path=os.path.join('csv', directory , f'{start_date_time.strftime(\"%Y%m%d\")}{filetype}.csv')\n",
    "    parquet_path=os.path.join('csv', directory , f'{start_date_time.strftime(\"%Y%m%d\")}{filetype}.parquet')\n",
    "    print(\"Converting csv to parquet\")\n",
    "    for i,chunk in enumerate(pd.read_csv(csv_path, chunksize=chunk_size)):\n",
    "        sys.stdout.write(\"Conversion progress: %d   \\r\" % (i) )\n",
    "        sys.stdout.flush()\n",
    "        parquet_writer = pd.concat([parquet_writer,chunk])\n",
    "    parquet_writer.to_parquet(parquet_path)\n",
    "    display(f\"{parquet_path}\")\n",
    "\n",
    "def parquet_to_csv(start_date_time, filetype=FileType.JOINED):\n",
    "    print(\"Converting parquet to csv\")\n",
    "    df1=load_df(start_date_time, filetype=filetype, parquet=True)\n",
    "    save_df(df1, start_date_time, filetype=filetype, parquet=False)\n",
    "\n",
    "\n",
    "class ResultGroup:\n",
    "    EQUIPMENT_FAILURE = \"equipment_failure\"\n",
    "    NORMAL = \"normal\"\n",
    "    ANALYSE = \"analyse\"\n",
    "    UNKNOWN = \"unknown\"\n",
    "\n",
    "# plot Alt/NIC vs Time\n",
    "def generate_plots(sample_size, group = ResultGroup.ANALYSE):\n",
    "    global df1\n",
    "    global flights_data\n",
    "\n",
    "    if isinstance(df1, type(None)) or isinstance(flights_data, type(None)):\n",
    "        print(\"Generate plots: error no data\")\n",
    "        return\n",
    "\n",
    "    flights = [str(flight) for flight in flights_data[flights_data['result']==group].sample(sample_size)['flight']]\n",
    "    for flight in flights:\n",
    "        flight_df = df1[df1['flight']==flight]\n",
    "        flight_df.set_index(\"time\")\n",
    "\n",
    "        fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "        ax1.plot(flight_df['time'].astype(\"datetime64[ns]\"), flight_df['alt_baro'], 'bx-', label='alt_baro')\n",
    "        ax1.tick_params('y', colors='b')\n",
    "        ax1.set_yticks(np.arange(0, 50000, 5000))\n",
    "        ax1.xaxis.set_major_formatter(mdates.DateFormatter('%d-%m-%Y %H:%M:%S'))\n",
    "\n",
    "        ax1.set_xlabel('Date-Time', fontsize=10)\n",
    "        ax1.set_ylabel('Altitude (ft)', color='b', fontsize=10)\n",
    "\n",
    "        plt.gcf().autofmt_xdate()\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(flight_df['time'].astype(\"datetime64[ns]\"), flight_df['nic'], 'rx-', label='nic')\n",
    "        ax2.set_ylabel('NIC', color='g', fontsize=10)\n",
    "        ax2.tick_params('y', colors='g')\n",
    "        ax2.set_yticks(np.arange(0, 14, 1))\n",
    "\n",
    "        plt.title(f'Flight:{flight} Altitude and NIC vs Time  (categorised as {group})', fontsize=10)\n",
    "        fig.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "        url = f\"https://doc8643.com/aircraft/{flight_df.iloc[0]['t']}\"\n",
    "        print(url)\n",
    "# categorise flights based on NIC\n",
    "def analyse_data(percentage_bad):\n",
    "    upper_limit=0.9\n",
    "    lower_limit=0.1\n",
    "    if percentage_bad>upper_limit:\n",
    "        return \"equipment_failure\"\n",
    "    elif percentage_bad<lower_limit:\n",
    "        return \"normal\"\n",
    "    elif percentage_bad>=lower_limit and percentage_bad<=upper_limit:\n",
    "        return \"analyse\"\n",
    "    else: \n",
    "        return \"unknown\"\n",
    "\n",
    "# correlations plot\n",
    "def generate_correlations(sample_size, group = ResultGroup.ANALYSE):\n",
    "    global df1\n",
    "    global flights_data\n",
    "\n",
    "    if isinstance(df1, type(None)) or isinstance(flights_data, type(None)):\n",
    "        print(\"Generate plots: error no data\")\n",
    "        return\n",
    "\n",
    "    flights = [str(flight) for flight in flights_data[flights_data['result']==group].sample(sample_size)['flight']]\n",
    "    flights_df=None\n",
    "    for flight in flights:\n",
    "        flight_df = df1[df1['flight']==flight]\n",
    "        flights_df = pd.concat([flights_df,flight_df])    \n",
    "    \n",
    "    flights_df.set_index(\"time\")\n",
    "    selected_columns = [\"alt_baro\", \"nic\", \"nac_p\",\"nac_v\",\"track\",\"baro_rate\",\"seen_pos\",\"seen\",\"gs\",\"alt_geom\"]\n",
    "    print(flights_df.shape[0])\n",
    "    display(flights_df.head(10))\n",
    "\n",
    "    corr_matrix = flights_df[selected_columns].corr(method='pearson')\n",
    "    ax = plt.axes()\n",
    "    ax.set_title(f'Pearson cofficient for sample size {sample_size}, \\ngroup: {group} \\nflights: {flights} ',fontsize=10)\n",
    "    sn.heatmap(corr_matrix, annot=True)\n",
    "    plt.show()\n",
    "\n",
    "# generate hexagons\n",
    "def get_hexagon_grid(latitude, longitude, resolution, ring_size):\n",
    "    \"\"\"\n",
    "    Generate a hexagonal grid GeoDataFrame centered around a specified location.\n",
    "    Parameters:\n",
    "    - latitude (float): Latitude of the center point.\n",
    "    - longitude (float): Longitude of the center point.\n",
    "    - resolution (int): H3 resolution for hexagons.\n",
    "    - ring_size (int): Number of rings to create around the center hexagon.\n",
    "    Returns:\n",
    "    - hexagon_df (geopandas.GeoDataFrame): GeoDataFrame containing hexagons and their geometries.\n",
    "    \"\"\"\n",
    "\n",
    "    global hexagon_df\n",
    "\n",
    "    # Get the H3 hexagons covering the specified location\n",
    "    center_h3 = h3.geo_to_h3(latitude, longitude, resolution)\n",
    "    hexagons = list(h3.k_ring(center_h3, ring_size))  # Convert the set to a list\n",
    "\n",
    "    # Create a GeoDataFrame with hexagons and their corresponding geometries\n",
    "    hexagon_geometries = [shapely.geometry.Polygon(h3.h3_to_geo_boundary(hexagon, geo_json=True)) for hexagon in hexagons]\n",
    "    hexagon_df = gpd.GeoDataFrame({'Hexagon_ID': hexagons, 'geometry': hexagon_geometries})\n",
    "    return hexagon_df\n",
    "\n",
    "# plot hexagon map    \n",
    "def create_choropleth_map(alpha=0.5, map_style=\"carto-positron\", data=\"percentage_bad\", limits=[0,0.1,0.5,1], lat=home_lat, lon=home_lng, hex_list=[]):\n",
    "    \"\"\"\n",
    "    Create an interactive choropleth map using Plotly Express.\n",
    "    Parameters:\n",
    "    - geojson_df (GeoDataFrame): GeoJSON data containing polygon geometries.\n",
    "    - data_df (DataFrame): DataFrame containing data to be visualized on the map.\n",
    "    - alpha (float): Opacity level for the map polygons (0.0 to 1.0).\n",
    "    - map_style (str): Map style for the Plotly map (e.g., \"carto-positron\").\n",
    "    - color_scale (str): Color scale for the choropleth map.\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    global df2\n",
    "    global hexagon_df\n",
    "    global flights_df\n",
    "    global hex_list_df\n",
    "    \n",
    "\n",
    "    # Hexagon grid around HOME\n",
    "    if isinstance(hexagon_df, type(None)):\n",
    "        hexagon_df = get_hexagon_grid(home_lat, home_lng, resolution, ring_size)\n",
    "    print(\"hexagons calculated\")\n",
    "\n",
    "    if isinstance(hex_list_df, type(None)):\n",
    "        hex_list_df = df2[(df2[\"Hexagon_ID\"].isin(hex_list))]\n",
    "    print(\"hex_list_df calculated\")\n",
    "    display(hex_list_df.head(10))\n",
    "\n",
    "    # Merge the GeoJSON data with your DataFrame\n",
    "    merged_df = hexagon_df.merge(df2 if (len(hex_list)==0) else hex_list_df, on=\"Hexagon_ID\", how=\"left\")\n",
    "    print(f'merged hexagon_df with {\"df2\" if (len(hex_list)==0) else \"hex_list_df\"}')\n",
    "\n",
    "    merged_df['lat'].astype(float)\n",
    "    merged_df['lon'].astype(float)\n",
    "    # merged_df = merged_df.dropna(subset=['lat', 'lon']) if merged_df[\"lat\"].isnull().values.any() else merged_df\n",
    "    \n",
    "    display(merged_df.head(10))\n",
    "    print(f\"merged_df: {merged_df.shape[0]}\")\n",
    "\n",
    "    # Create a choropleth map using px.choropleth_mapbox\n",
    "    fig = px.choropleth_mapbox(\n",
    "        merged_df,\n",
    "        geojson=merged_df.geometry,\n",
    "        locations=merged_df.index,  # Use index as locations to avoid duplicate rows\n",
    "        color=data,\n",
    "        color_continuous_scale=[[limits[0], f'rgba(0,255,0,{alpha})'],\n",
    "                                [limits[1], f'rgba(255,255,0,{alpha})'],\n",
    "                                [limits[2], f'rgba(255,0,0,{alpha})'],\n",
    "                                [limits[3], f'rgba(255,0,0,{alpha})']],        \n",
    "        title=\"GPS Jam Map\",\n",
    "        mapbox_style=map_style,\n",
    "        center={\"lat\": lat, \"lon\": lon},  # Adjust the center as needed\n",
    "        zoom=2,\n",
    "    )\n",
    "\n",
    "    # Customize the opacity of the hexagons\n",
    "    fig.update_traces(marker=dict(opacity=alpha))\n",
    "\n",
    "    # Add hover data for hotel names\n",
    "    fig.update_traces(customdata=merged_df[[\"Hexagon_ID\",\"bad_count\", \"total_count\", \"percentage_bad\", \"lat\", \"lon\"]])\n",
    "\n",
    "    # Define the hover template \n",
    "    hover_template = \"<b>Hexagon ID:</b> %{customdata[0]}<br><b>Location:</b> %{customdata[4]:.4f},%{customdata[5]:.4f}<br><b>Percentage bad:</b> %{customdata[3]:.3f}<br><b>Total Count:</b> %{customdata[2]}<extra></extra>\"\n",
    "    fig.update_traces(hovertemplate=hover_template)\n",
    "\n",
    "    # Set margins to 25 on all sides\n",
    "    fig.update_layout(margin=dict(l=35, r=35, t=45, b=35))\n",
    "    \n",
    "    # Adjust the width of the visualization\n",
    "    fig.update_layout(width=1000) \n",
    "\n",
    "\n",
    "    # Create a scatter mapbox plot for the flight data\n",
    "    flight_fig = px.scatter_mapbox(\n",
    "        flights_df,\n",
    "        lat=\"lat\",\n",
    "        lon=\"lon\",\n",
    "        hover_name=\"flight\", \n",
    "        hover_data=[\"flight\",\"time\",\"alt_baro\", \"nic\", \"nac_p\",\"nac_v\",\"track\",\"baro_rate\",\"seen_pos\",\"seen\",\"gs\",\"alt_geom\"], \n",
    "        color_discrete_sequence=[\"blue\"],  # Set the color of the flight points\n",
    "        zoom=2,\n",
    "    )\n",
    "\n",
    "    # Overlay the flight data onto the choropleth map\n",
    "    fig.add_trace(flight_fig.data[0])\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# plot flights on hexagon map, only those flights passing through hexagons in the hex_list\n",
    "def get_flights_with_hex_list(hex_list):\n",
    "        \n",
    "        global flights_df\n",
    "        global df1\n",
    "\n",
    "        flights_df=None\n",
    "\n",
    "\n",
    "        for hexagon_ID in hex_list:\n",
    "                \n",
    "                # get the flights in that hexagon\n",
    "                flights_in_hex = df1[df1['Hexagon_ID']==hexagon_ID]['flight'].unique()\n",
    "\n",
    "                # extract all the rows for those flights\n",
    "                for flight in flights_in_hex:\n",
    "                        flight_df = df1[(df1['flight']==flight) & (df1['Hexagon_ID']==hexagon_ID)]\n",
    "                        time_start = flight_df[\"time\"].min()\n",
    "                        time_end = flight_df[\"time\"].max()\n",
    "                        # flight_df2 contains all pings of the flight between the start and end time that the flight is inside the hex. i.e. it includes pings that have null lat/lon values\n",
    "                        flight_df2 = df1[(df1['flight']==flight) & (df1['time'].between(time_start, time_end))]\n",
    "                        flights_df = pd.concat([flights_df,flight_df2])\n",
    "\n",
    "        display(flights_df.head(5))\n",
    "\n",
    "        return flights_df\n",
    "        # plot choropleth map with flights overlaid on top of the hexagons\n",
    "        create_choropleth_map(data=\"percentage_bad\", limits = [0,0.1,0.5,1], lat=hex_list[0][0], lon=hex_list[0][1], hex_list=hex_list)\n",
    "\n",
    "\n",
    "# note that for the free sample, only the first day of each month is available on the adsbexchange\n",
    "# 4 hours + to download 1 day of files\n",
    "# 18 mins to reload\n",
    "\n",
    "[df1, flights_data, df2] = process_data(dt.datetime(2024,4,1,0,0,0),dt.datetime(2024,4,2,0,0,0), reload=[False, False, False])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief \n",
    "The dashboard or app has to be useful for a flight planner to decide how to plan a mission or for a pilot to decide how he might want to fly a flight profile. Given this, how should we investigate the data to determine a representation that yould help him/her. With this in mind, what does the data tell us? GPS Jamming is like encountering turbulence on route. It's undesirable for flight, but it's usually not a safety hazard unless it's large and unexpected. \n",
    "It is terrible for munitions that rely on this for guidance. But these profiles can be adjusted. You have access to pilots and planners who would fly these things. So we'll help to provide some context to how the data may be investigated. But do look at the raw data and see what it can tell you. \n",
    "Maybe you want to try unsupervised learning to see if there are distinct categories. And then investigate what are the common factors in these clusters? Do the EDA and see what hidden insights there are. Then you can decide on the best representation of the data.\n",
    "\n",
    "### Exploratory Questions \n",
    " 1. Sources of Truth: What are the sources of truth for GPS interference? \n",
    " 2. Correlation: How is GPS signal affected by altitude? Speed? Is there any strong correlation?\n",
    " 3. Analysis: Select some sample flights within an area and examine the GPS drop off characteristics. Select a few known GPS jammed/spoofed areas and a known GPS safe area as a baseline. What does the data tell us?\n",
    " 4. Classification: From what is learnt in item 2, what algorithm can we use to classify the area on GPS interference? \n",
    " 5. Presentation: What data is useful to pilots when describing an area of GPS interference? What is the best way to present this data?\n",
    "\n",
    "### Sources of Truth - What are the sources of truth for GPS interference?\n",
    "1. ADSB api is certainly one of them. \n",
    "2. Flight plans are not followed exactly so comparing flight plans with actual paths is not reliable. \n",
    "3. Radar returns are another source of truth, but these are not readily available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df(df2, filetype=FileType.HEXBIN, parquet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Plots For Sampled Flights, Alt/NIC vs Time\n",
    "# start_date_time = dt.datetime(2024,4,1,0,0,0)\n",
    "# df1 = load_df(start_date_time, filetype=FileType.JOINED, parquet=True)\n",
    "# flights_data = load_df(start_date_time, filetype=FileType.FLIGHTS_DATA, parquet=True)\n",
    "# display(flights_data['result'].value_counts())\n",
    "\n",
    "# > 0.9: Equipment Failure\n",
    "# Between 0.1 and 0.9: Analyse\n",
    "# < 0.1: Normal\n",
    "# generate_plots(5, group=ResultGroup.NORMAL)\n",
    "generate_plots(5, group=ResultGroup.ANALYSE)\n",
    "# generate_plots(5, group=ResultGroup.EQUIPMENT_FAILURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlations\n",
    "# start_date_time = dt.datetime(2024,4,1,0,0,0)\n",
    "# df1 = load_df(start_date_time, filetype=FileType.JOINED, parquet=True)\n",
    "# flights_data = load_df(start_date_time, filetype=FileType.FLIGHTS_DATA, parquet=True)\n",
    "\n",
    "generate_correlations(20, group = ResultGroup.ANALYSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df2.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_time = dt.datetime(2024,4,1,0,0,0)\n",
    "df1 = load_df(start_date_time, filetype=FileType.JOINED, parquet=True) if isinstance(df1, type(None)) else df1\n",
    "flights_data = load_df(start_date_time, filetype=FileType.FLIGHTS_DATA, parquet=True) if isinstance(flights_data, type(None)) else flights_data\n",
    "\n",
    "sample = flights_data[flights_data[\"result\"]==ResultGroup.ANALYSE].sample(5)\n",
    "sample_df=None\n",
    "display(sample.head(5))\n",
    "for flight in sample[\"flight\"]:\n",
    "    sample_df = pd.concat([sample_df,df1[df1[\"flight\"]==flight]])\n",
    "display(sample_df[(sample_df[\"flight\"].isin(sample[\"flight\"])) & sample_df['lat'].isnull()])\n",
    "sample_df2 = calculate_hexagon_ids(sample_df[sample_df[\"flight\"].isin(sample[\"flight\"])])\n",
    "sample_df2=sample_df2[~sample_df2['lat'].isnull()]\n",
    "display(sample_df2[sample_df2['lat'].isnull()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Hex Map\n",
    "\n",
    "start_date_time = dt.datetime(2024,4,1,0,0,0)\n",
    "df1 = load_df(start_date_time, filetype=FileType.JOINED, parquet=True) if isinstance(df1, type(None)) else df1\n",
    "# display(df1.head(5))\n",
    "print(\"Get hexagon df\")\n",
    "df2 = load_df(start_date_time, filetype=FileType.HEXBIN, parquet=True) if isinstance(df2, type(None)) else df2\n",
    "df2 = df2[df2[\"Hexagon_ID\"]!=\"NA\"]\n",
    "display(df2.tail(5))\n",
    "print(f\"df2: {df2.shape[0]}\")\n",
    "# save_df(df2, start_date_time, filetype=FileType.HEXBIN, parquet=True)\n",
    "flights_data = load_df(start_date_time, filetype=FileType.FLIGHTS_DATA, parquet=True) if isinstance(flights_data, type(None)) else flights_data\n",
    "# display(flights_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coordinates of all the suspected GPS jammed hexagons\n",
    "coord_list = [[33.81825, 35.49082],[30.0842, 31.35159]]\n",
    "#convert to hexagon_ID\n",
    "hex_list = [h3.geo_to_h3(coord[0], coord[1], resolution=resolution) for coord in coord_list]\n",
    "print(hex_list)\n",
    "\n",
    "# extract flights that pass through those hexagons\n",
    "flights_df = get_flights_with_hex_list(hex_list) if isinstance(flights_df, type(None)) else flights_df\n",
    "\n",
    "create_choropleth_map(data=\"percentage_bad\", limits = [0,0.1,0.5,1], lat=coord_list[0][0], lon=coord_list[0][1], hex_list=hex_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given lat/lon -> find hexagon_ID -> filter all flights\n",
    "# plot flights on hexagon map\n",
    "\n",
    "# plot Altitude / NIC vs time\n",
    "\n",
    "# 2024-04-01\n",
    "# 33.81825, 35.49082\n",
    "# Label 318/Size 63600\n",
    "# 30.0842, 31.35159\n",
    "# Label 281/Size 56200\n",
    "\n",
    "\n",
    "# 2024-02-01\n",
    "# 33.81825, 35.49082\n",
    "# Label 200/Size 40000\n",
    "\n",
    "# 2024-03-01\n",
    "# 33.81825, 35.49082\n",
    "# Label 120/Size 24000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Change in NIC/NAC/SIL indicates an anomaly, which may be due to any reason\n",
    "\n",
    "# nic: Navigation Integrity Category (2.2.3.2.7.2.6)\n",
    "# Table 1: NIC value and corresponding size of containment radius\n",
    "# NIC Containment Radius\n",
    "# 0 Unknown\n",
    "# 1 Rc < 37.04km (20nm)\n",
    "# 2 Rc < 14.816km (8nm)\n",
    "# 3 Rc < 7.408km (4nm)\n",
    "# 4 Rc < 3.704km (2nm)\n",
    "# 5 Rc < 1852m (1nm)\n",
    "# 6 Rc < 1111.2m (0.6nm)\n",
    "# Rc < 926m (0.5nm)\n",
    "# Rc < 555.6m (0.3nm)\n",
    "# 7 Rc < 370.4m (0.2nm)\n",
    "# 8 Rc < 185.2m (0.1nm)\n",
    "# 9 Rc < 75m\n",
    "# 10 Rc < 25m\n",
    "# 11 Rc < 7.5m\n",
    "\n",
    "# Assuming you have already calculated the counts\n",
    "# counts = df1['nic'].value_counts(dropna=False)\n",
    "# print(f\"List all nic values with counts {counts}\")\n",
    "# # Create bins for the histogram\n",
    "# bins = np.arange(len(counts) + 1)\n",
    "\n",
    "# # Plot the histogram with NA on the left\n",
    "# plt.hist(df1['nic'], bins=bins, align='left',rwidth=0.5)\n",
    "# plt.yscale('log')\n",
    "# plt.title('nic')\n",
    "# plt.show()\n",
    "\n",
    "print(f\"bad {df1[df1['good_bad']=='bad'].shape[0]}\")\n",
    "print(f\"total {df1.shape[0]}\")\n",
    "print(f\"% bad / total {df1[df1['good_bad']=='bad'].shape[0]/df1.shape[0]*100:.2f}%\")\n",
    "df1[df1['good_bad']=='bad'].head(100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "print(f\"List all type with counts {df1['type'].value_counts()}\")\n",
    "# List all type with counts type\n",
    "# adsb_icao         9145 - messages from a Mode S or ADS-B transponder, using a 24-bit ICAO address\n",
    "# other              725 - IGNORE miscellaneous data received via Basestation / SBS format, quality / source is unknown.\n",
    "# adsb_icao_nt       524 - IGNORE - messages from an ADS-B equipped “non-transponder” emitter e.g. a ground vehicle, using a 24-bit ICAO address\n",
    "# mode_s             515 - ModeS data from the planes transponder (no position transmitted)\n",
    "# adsr_icao          280 - rebroadcast of ADS-B messages originally sent via another data link e.g. UAT, using a 24-bit ICAO address\n",
    "# tisb_other         256 - traffic information about a non-ADS-B target using a non-ICAO address\n",
    "# tisb_trackfile     214 - traffic information about a non-ADS-B target using a track/file identifier, typically from primary or Mode A/C radar\n",
    "# mlat               116 - MLAT, position calculated arrival time differences using multiple receivers, outliers and varying accuracy is expected.\n",
    "# unknown             49\n",
    "# tisb_icao           31 - traffic information about a non-ADS-B target identified by a 24-bit ICAO address, e.g. a Mode S target tracked by secondary radar\n",
    "# adsb_other          17 - messages from an ADS-B transponder using a non-ICAO address, e.g. anonymized address\n",
    "plt.hist(df1['type'], align='left')\n",
    "plt.title('type')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# category: emitter category to identify particular aircraft or vehicle classes (values A0 – D7) (2.2.3.2.5.2)\n",
    "counts = df1['category'].dropna()\n",
    "# print(f\"List all category with counts {counts}\")\n",
    "plt.hist(counts)\n",
    "plt.title('category')\n",
    "plt.show()\n",
    "# A0 : No ADS-B emitter category information. Do not use this emitter category. If no emitter category fits your installation, seek guidance from the FAA as appropriate. A1 : Light (< 15500 lbs) – Any airplane with a maximum takeoff weight less than 15,500 pounds. This includes very light aircraft (light sport aircraft) that do not meet the requirements of 14 CFR § 103.1.\n",
    "# A2 : Small (15500 to 75000 lbs) – Any airplane with a maximum takeoff weight greater than or equal to15,500 pounds but less than 75,000 pounds.\n",
    "# A3 : Large (75000 to 300000 lbs) – Any airplane with a maximum takeoff weight greater than or equal to 75,000 pounds but less than 300,000 pounds that does not qualify for the high vortex category.\n",
    "# A4 :  High vortex large (aircraft such as B-757) – Any airplane with a maximum takeoff weight greater than or equal to 75,000 pounds but less than 300,000 pounds that has been determined to generate a high wake vortex. Currently, the Boeing 757 is the only example.\n",
    "# A5 : Heavy (> 300000 lbs) – Any airplane with a maximum takeoff weight equal to or above 300,000 pounds.\n",
    "# A6 : High performance (> 5g acceleration and 400 kts) – Any airplane, regardless of weight, which can maneuver in excess of 5 G’s and maintain true airspeed above 400 knots.\n",
    "# A7 : Rotorcraft – Any rotorcraft regardless of weight.\n",
    "# B0 : No ADS-B emitter category information\n",
    "# B1 : Glider / sailplane – Any glider or sailplane regardless of weight.\n",
    "# B2 : Lighter-than-air – Any lighter than air (airship or balloon) regardless of weight.\n",
    "# B3 : Parachutist / skydiver\n",
    "# B4 : Ultralight / hang-glider / paraglider – A vehicle that meets the requirements of 14 CFR § 103.1. Light sport aircraft should not use the ultralight emitter category unless they meet 14 CFR § 103.1.\n",
    "# B5 : Reserved\n",
    "# B6 : Unmanned aerial vehicle – Any unmanned aerial vehicle or unmanned aircraft system regardless of weight.\n",
    "# B7 : Space / trans-atmospheric vehicle\n",
    "# C0 : No ADS-B emitter category information\n",
    "# C1 : Surface vehicle – emergency vehicle\n",
    "# C2 : Surface vehicle – service vehicle\n",
    "# C3 : Point obstacle (includes tethered balloons)\n",
    "# C4 : Cluster obstacle\n",
    "# C5 : Line obstacle\n",
    "# C6 : Reserved\n",
    "# C7 : Reserved\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes to Myself (Hope some are useful to other devs)\n",
    "\n",
    "#### Strategies for Sanity or \"How do I do this data thingy with less frustration?\"\n",
    "1. Massive, massive data! How not to crash your computer... \n",
    "   - Get the method right before scaling. Load the data with only 1 hour instead of 24 hours. Once you have debugged your code and it works well, then scale up to longer duration. \n",
    "   - Filter the columns, discard what is not useful.\n",
    "   - Save and load from file instead of keep downloading.\n",
    "\n",
    "2. Dirty data crashing your app.\n",
    "   - Be careful of wrong data types (float vs string) \n",
    "   - Be careful of NA and NaN (not a number) \n",
    "   - Check column names carefully (lon vs lng, Hexagon_ID vs hexagon_ID ... I know, my bad) \n",
    "\n",
    "3. How to speed up? It is taking too long! \n",
    "    - Be patient, we have to wait for good things to happen. Downloading 1 day of ADSB data takes 4 hours+ \n",
    "    - Use a powerful computer with lots of RAM and disk space, preferably a server. \n",
    "    - Load only the data that you need, not easy because how to know which data we want to see if we cannot see them? Use .info(), .describe() .nunique() .head() .tail() .sample() to do quick summary of the massive data.  \n",
    "    - Use Parquet format instead of csv to read and save the files. It is more performant and uses compression. Only downside is we cannot inspect it like a csv. \n",
    "\n",
    "4. Some lat lon are NaN because of GPS failure. \n",
    "    - To assign nearest hexagon_ID based on nearest time with lat lon. \n",
    "    - This can be quite trickly as some drops are on the ground, some are for too long duiration to get the nearest valid coordinate is difficult. \n",
    "\n",
    "#### DO this in Pandas\n",
    "1. .info() to get data types  or .dtypes  \n",
    "2. .describe() 0> gets mean() count(), 25%, 50%, 75% max min\n",
    "3. .isnull() check for missing values   ->  isnull().sum().sort_values(ascending=False)\n",
    "4. .nunique() -> gets unique values for each field with their counts\n",
    "5. .unique() -> gets a list of unique values in a column\n",
    "6. .value_counts() -> gets unique values for the column with their counts\n",
    "7. for name, group in grouped: -> iterate over groups\n",
    "8. pandas time series functions: Timestamp->DatetimeIndex[] (to_datetime, date_range), Timedelta->TimedeltaIndex[] (to_timedelta, timedelta_range), Period->PeriodIndex[] (Period, period_range), DateOffset (Dateoffset)\n",
    "9. Missing Values: Numeric (pd.nan), String (pd.NA), Time (pd.NaT) -> use .isna(), .notna() to detect missing values\n",
    "10. isinstance(df1, type(None)) to check for empty dataframe\n",
    "11. df1['lat'].astype(float) to convert to type float\n",
    "12. df1[(df1['nic']<7) & (df1['flight']=flight)] for dataframe conditional logic use bitwise and '&' or '|' not '~'\n",
    "\n",
    "#### DON'T DO this in Pandas \n",
    "1. Seems like saving as a pickle may corrupt the data, I see a lot of NA, maybe due to the data conversion between types. avoid any data conversion except at the start. UPDATE: moved on to using Parquet (faster and safer and saves memory)\n",
    "2. Use display() instead of print() -> formats the dataframes nicely\n",
    "3. .dropna() -> WRONG USE, drops any row or column with a missing value!\n",
    "4. df[df[]] is a filter, not selecting the rows. it is wrong! To filter the rows use df[['col1','col2',...]]\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
